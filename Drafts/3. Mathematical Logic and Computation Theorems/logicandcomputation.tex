\documentclass[10pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{url}
\usepackage{makeidx}
\usepackage{graphicx}
\usepackage{graphicx, adjustbox}
\usepackage{lmodern}
\usepackage{fourier}
\usepackage{float}
\usepackage{caption}
\usepackage{wrapfig}
\usepackage{mhchem}
\usepackage[left=1.5cm,right=1.5cm,top=1cm,bottom=3cm]{geometry}
\usepackage{multicol}
\usepackage{soul}



%Colors
\usepackage[dvipsnames]{xcolor}


\definecolor{black}{RGB}{0, 0, 0}
\definecolor{richblack}{RGB}{7, 14, 13}
\definecolor{charcoal}{RGB}{45, 67, 77}
\definecolor{delectricblue}{RGB}{93, 117, 131}
\definecolor{cultured}{RGB}{245, 245, 245}
\definecolor{lightgray}{RGB}{211, 216, 218}
\definecolor{silversand}{RGB}{190, 194, 198}
\definecolor{spanishgray}{RGB}{148, 150, 157}
\definecolor{darkliver}{RGB}{64, 63, 76}

\colorlet{lightdelectricblue}{delectricblue!30}
\colorlet{lightdarkliver}{darkliver!30}


%ColorDefines
\newcommand{\trueblack}[1]{\textcolor{black}{#1}}
\newcommand{\rich}[1]{\textcolor{richblack}{#1}}
\newcommand{\lightblack}[1]{\textcolor{charcoal}{#1}}
\newcommand{\lightrich}[1]{\textcolor{delectricblue}{#1}}


%Boxes
\usepackage{tcolorbox}
\newtcolorbox{calloutbox}{center,%
    colframe =red!0,%
    colback=cultured,
    title={Callout},
    coltitle=richblack,
    attach title to upper={\ ---\ },
    sharpish corners,
    enlarge by=0.5pt}

\newtcolorbox[use counter=equation]{eq}{center,
	colframe =red!0,
	colback=cultured,
	title={\thetcbcounter},
	coltitle=richblack,
	detach title,
	after upper={\par\hfill\tcbtitle},
	sharpish corners,
    enlarge by=0.5pt }
    
\newtcolorbox{qt}{center,
	colframe=delectricblue,
	colback=white!0,
	title={\large "},
	coltitle=delectricblue,
	attach title to upper,
	after upper ={\large "},
	sharp corners,
	enlarge by=0.5pt,
	boxrule=0pt,
	leftrule=2pt}
	
\newtcolorbox{exc}{center,%
    colframe =red!0,%
    colback=darkliver!15,
    title={Excercise},
    coltitle=richblack,
    attach title to upper={\ ---\ },
    sharpish corners,
    enlarge by=0.5pt}
    
\newcounter{theo}
\newtcolorbox[use counter=theo]{theorem}
	{center,%
    colframe =red!0,%
    colback=cultured,
    title={Theorem \thetcbcounter},
    coltitle=richblack,
    attach title to upper={\ ---\ },
    sharpish corners,
    enlarge by=0.5pt}

\newcounter{defcounting}
\newtcolorbox[use counter=defcounting]{define}
{center,%
	colframe=darkliver!50,%
	colback=white!0,
	title={\textcolor{black}{\textbf{\textit{Definition}} \  \thetcbcounter  \ --}},
	coltitle=darkliver!50,
	attach title to upper,
	after upper ={ },
	sharp corners,
	enlarge by=0.5pt,
	boxrule=0pt,
	leftrule=2pt,
    rightrule = 0pt}

\newcounter{lemmacount}
\newtcolorbox[use counter=lemmacount]{lemma}
{center,%
    colframe=charcoal!50,%
    colback=white!0,
    title={\textcolor{black}{\textbf{\textit{Lemma}} \  \thetcbcounter  \ --}},
    coltitle=darkliver!50,
    attach title to upper,
    after upper ={ },
    sharp corners,
    enlarge by=0.5pt,
    boxrule=2pt}
    

\newcounter{examplecounter}
\newtcolorbox[use counter=examplecounter]{example}
	{center,%
    colframe =red!0,%
    colback=cultured,
    title={Example},
    coltitle=richblack,
    attach title to upper={\ ---\ },
    sharpish corners,
    enlarge by=0.5pt}

    

        
    
% Highlighters
\newcommand{\hldl}[1]{%
	\sethlcolor{lightdarkliver}%
	\hl{#1}
}
\newcommand{\hldb}[1]{%
    \sethlcolor{lightdelectricblue}%
    \hl{#1}%
}


% Images
\newcounter{figurecounter}
\setcounter{figurecounter}{1}

\newcommand{\img}[3]{
    \begin{figure}[h!]
        \centering
        \captionsetup{justification=centering,margin=0cm,labelformat=empty}
        \includegraphics[width=#2\linewidth]{./img/#1}
        \label{figure}
        \caption{\small\textbf{fig-\thefigurecounter} -- \textcolor{darkliver}{#3}}
    \end{figure}
    \addtocounter{figurecounter}{1}}

\newcommand{\imgr}[3]{
    \begin{wrapfigure}{r}{#2\textwidth}
        \centering
        \captionsetup{justification=centering,margin=0cm,labelformat=empty}
        \includegraphics[width=\linewidth]{./img/#1}
        \label{figure}
        \caption{\small \textbf{fig: \thefigurecounter} -- \textcolor{darkliver}{#3}}
    \end{wrapfigure}
    \addtocounter{figurecounter}{1}}

\newcommand{\imgl}[3]{
    \begin{wrapfigure}{l}{#2\textwidth}
        \centering
        \captionsetup{justification=centering,margin=0cm,labelformat=empty}
        \includegraphics[width=\linewidth]{./img/#1}
        \label{figure}
        \caption{\small \textbf{fig: \thefigurecounter} -- \textcolor{darkliver}{#3}}
    \end{wrapfigure}
    \addtocounter{figurecounter}{1}}

% New commands
\newenvironment{callout}
	{\begin{calloutbox}\color{charcoal}\textbf\textit}
	{\end{calloutbox}}

% for this file
\newcommand{\newpoint}[1]{\indent$\mathsection$ \textbf{#1}}
\newcommand{\curveL}{\mathcal{L}}
\newcommand{\curveA}{\mathcal{A}}
\newcommand{\curveP}{\mathcal{P}}
\newcommand{\thm}{\text{Thm}}
\newcommand{\proof}{\\ \ \\ $\blacktriangleright$ \textit{proof: }}

\title{Mathematical Limits \\ \large Logic and Computational Theorems}
\author{Amir H. Ebrahimnezhad}
\date{\today}
\begin{document}
        \maketitle
        \tableofcontents
        \newpage
        
        \section{Ground Works}
            \subsection{Vorwort}
              In this paper we would work our way to understand basic epistemology, mathematical theory of formal systems, the way we prove statements and deduction as pure mathematical descriptions, incompleteness theorems and computational limits. Then we would talk about measurement and the similar behavious by nature and what the incompleteness theorem would suggest. 
        \section{What is Epistemology?}
                \indent At the core of the science there's always a simple question to be answered, the question that needs to be asked and properly investigated before any kind of scientific advancement is acheived. That is \textit{"Why are we sure about the knowledge we have, and what is it after all?"}. The boundaries of science are small to hold such a question within, since they infact are the product of it themselves.
                \\
                \\
                It seems to be a good place to start, since the question of the whole research lies upon the statement that \textit{maybe} it is not possible to know everything about the universe, where we have to first define what we mean by everything. But knowledge itself is where we begin.
                \\
                \\
                \indent Epistemology, concerns itself about the problems and theories regarding knowldege. The word is derived from the Greek words \textit{epistéme} and \textit{logos}, which together means the study of knowledge. But to even begin with such philosophy one must try to define first hand:
                \begin{itemize}
                    \item What is knowledge, and what do we mean when we say that we know something?
                    \item What is the source of knowledge, how do we gain reliable information and consider them as knowledge?
                    \item Is absolute knowledge possible? If not, what are the limitation?\cite{CW/E}
                \end{itemize}
                \indent The first question, seems to be a matter of definition, but an important roleis being played by asking about \textit{"What knowledge is?"}. The importance of the question arises from the fact that by defining knowledge carelessly we might include falsehood with the truth. which by any good considerations, is the last thing, which one in search of knowing would intend to do. Beside that if you define knowledge in a careless manner, you get in trouble to argue for good strategies and sources, and even not be able to find true limitations of knowledge.
                \\
                \\
                \indent The second question concerns us to think about methods, with which we gain information (false or true premises) about anything. What makes a method reliable and other don't. This qeustion incudes the old fashioned problem \textit{Why should we trust science?}, with this question I'll try to show that science, and specifically the process of experimenting is found to be the most reliable way to produce knowledge.
                \\
                \\
                \indent The last question is rather the aim of the project in front of you. This questions invites the careful study of the source of knowledge to be more specific, to show if it has any boundaries, or is it an endless tunnel of ever comming knowledge. We may what to argue, or more clearly, philosophize about the topic. Where we investigate the logic of the world, Computation, and mathematical view of nature and experience.
            
            \section{Knowledge and Justification}
                \subsection{Absilute Knowledge} The idea starts with the question, is there an absolute knowledge, and if so is it possible to gain it. Parmenide wanted the idea to be true, and for being so, he describes that knowledge should not depend upon changing observations and experiences, because it has its sole origin in the logic of rational thought; A knowledge that is to assure the experiences but at the same time, a knowledge that is given a priori and is conditioned by nothing but itself. \textit{A Knowledge that can claim for itself absolute centainty and validity.}
                \\
                \\
                Parmendise could be the first to attain the concept that it might be possible to grasp the absolute knowledge of the world, with the vision that the phenomenal reality is merely the deceptive illusiveness of a true and unchangeable worl. This hidden world, was believed to be accessible only by pure reasoning.
                \begin{qt}
                    Thus, the idea coagulated that true knowledge of the world could only be arrived at by following the path of rational thought,
                \end{qt}
                The weak point in the ontology of Parmenides was already pointed out by Aristotle. Instead Aristotle introduced a distinction between that which is \textit{"actuality"} and that which might be \textit{"potentiality"}. This he ultimately raised to a point of departure for his own metaphysics, in which he differs clearly from that of the Eleates.
                \\
                \\
                But as Parmendise intended the concept of \textit{true being, that is only accessible through rational thought} was sustained. We'll turn back again after some basic investigation.\cite{Kuppers2018-vv}
    
                \newpoint{Cognitive Success} is a term used to describe the ability of an individual to think, reason, learn, and solve probelms effectively. The ability to solve problems, and find the true values to things we seek, is a complex process, requiring one's mind to adjust, learn, be creative and manipulate information in a way that is actually useful to solve a problem. But despite that, one can easily argue that if you got the wrong information, false premises and false statements. No amount of intelligent process (without considering luck) would be able to make a useful prediction, or any effective progress toward one's goal. In fact this is known as a motto in data science \textit{Garbage in, Garbage out.}\cite{sep-epistemology}\cite{Clegg2017-ev}
                \\
                \\
                Therefore it is safe to say: \textbf{\textit{By any process, of which we receive information from, we seek statements that are true}}. From here we first have to define a true statement (knowledge) which is the first question posed in the introduction.
                \begin{callout}
                    It is worth to note that since this is the study of science, we might not consider all the possible ways one might use \textit{knowledge}. One might know someone, know how to do something, etc... Although one can argue that these concepts are also a higher conceptions of just basic facts (one might know how to do something because he understands basic factual statements of the system and prepared a path to follow, which, because of the facts beneath, happens to reach the desiered goal). We would only talk about, things we consider to be facts, in it's scientific term. (i.e earth is orbiting sun.).
                \end{callout}
                    \subsection{Defining Knowldege} We have different opinion in different areas of our lives and works, we might have an opinion about who is going to be the president later this year, or if the stocks are going to be bullish or bearish next week; Although we are able to hold any opinion and belief in our mind, we might like to be able to categorize them by some statements.\cite{CW/E}\cite{sep-epistemology}
                    \\
                    \\
                    \subsubsection{Validity} The first way to characterize an statement is the validity. It is safe to assume that we desire statements that we believe to be true. Consider the following statement:
                    \begin{align*}
                        \text{Gravity is described by Newton's law}
                    \end{align*}
                    The statement is true. Not always but if we have an accuracy of a 1700s' scientist, it is most certainly a true statement about the gravity. I would here propose that when we talk about the validity of an statement we might like to consider how accurate are we talking. For that the statement was considered to be a true statement for centuries; Now it is considered true but only if we change a little:
                    \begin{align*}
                        \text{In the limit of small velocities (with small accuracies)...}
                    \end{align*}
                    The validity of the statemen changed over time, it might happen to any statement, for instance if you believe that it's raining outside, you might find it true or false. This is a problem, not only you might find contradiction with what you hold as a belief. But the worst is yet to come, there can be scenarios where you are evaluating an statement correctly (you might be right about the weather), but it just happens to be a lucky guess. 
                    \\
                    \\
                    Certainly we would like true statements we hold, which are not evaluated true by mere luck, to be considered as knowledge. This would lead us to the second characteristic of knowledge.
                    \\
                    \\
                    \subsubsection{Justified} When Alice and Bob say that it's raining outside, where Alice just guessed, and Bob have looked through the window and actually saw the raining. One must consider the two ways, upon which they stated the condition of weather, differently. The first is unable to answer, \textit{"Why do you believe that it's raining?"}, while the latter would.
                    \\
                    \\
                    Assume that Alice and Bob always hold believes, by the way proposed, Alice only guesses, and Bob tries to justify what he considers true and if there's no justification, he would simply change his mind. Now if you are to use the information from one of them, who would you choose? A logical answer would be to always ask Bob, since there's atleast and arguement upon which he considers the belief to be true. 
                    \\
                    \\
                    Knowledge should be justifiable, this is more than just having good excuses to believe something, because it also helps the process of finding truth working, believing without justification cannot be questioned properly (other than questioning the unquestionability itself.). Being justifiable helps us to use the socratic method, either we derive an unquestionable fact underneath, or we find another belief which can or cannot be justified. Therefore it seems that \textit{Knowledge is Justified True Belief.}
                    \\
                    \\
                    But there are problems with such statement, since the justification condition was added to ensure that the belief is not true merely because of luck. For instance believing you have lung cancer, because an astrology magazine suggests, would be considered not justified from a scientist prespective but justified if you believe in astrology.
                    \\
                    \\
                    Edmun Gettier, showed that there are cases of \textit{Justified True Belief} that are not cases of knowledge. JTB, therefore, is not sufficient for knowledge. Cases that this is the case are known as the Gettier cases, which arise because neither the possession of adequate evidence, nor origination in reliable faculties, nor the conjunction of these conditions, is sufficient for ensuring that a belief is not true merely because of luck. This suggests that we must add another element to JTB, so that it is sufficient to be considered Knowledge.\cite{sep-epistemology}
                    \\
                    \\
                    \subsection{Defining Justification}
                    Imagine a situation where a kid, despite having a birth certificate, and what he has been told his entire life, were to find that the parents he thought are not his actual parents. This situation shows that although a belief was justified, it ultimately came to be wrong. Debates concerning the nature of justification can be understood as debates concernin the nature of such non-knowledge-guaranteeing cognitive successes as the one this imaginary kid would enjoy.
                    \\
                    \\
                    The term justification is used as to say \textit{under no obligation to refrain.} This definition of understanding is labeled as \textit{Dentological Justification} we can define:
                    \begin{define}
                        $S$ is justified in doing $x$ if and only if $S$ is not obliged to refrain from doing $x$.
                    \end{define}
                    So for the term justification we would define:
                    \begin{define}
                        $S$ is justified in believing that $p$ if and only if $S$ is not oblied to refrain from believing that that $p$.
                    \end{define}
                    The dentological understanding of the concept of justification is common among philosophers such as, Descartes, Locke, Moore and Chisholm.
                    \\
                    \\
                    Dentological justification is commonly used, \textit{"Inocent until proven guily"} is an obvious example in law, where we are assumming the most common assertion (people are mostly inocent) until there's an evidence to support otherwise. But such generalization is not the case in science, or to be more specific, until no evidence is gathered, we cannot put our finger on where the logical place to stand is. Although it is important to get back to dentological justification in science. We'll cover the use of it later.
                    \\
                    \\
                    But on the other hand we can define another type for justification:
                    \begin{define}
                        $S$ is justified in believing that $p$ if and only if $S$ believes that $p$ in a way that makes it sufficiently likely that her belief is true.
                    \end{define}
                    Dentological justification, though promising, lacks an important concept, where the justification should be correlated with the evaluation of the belief, one can believe in a justified manner (dentologically), but nevertheless his/her belief is false. The problem arises since dentological justification asserts true until proven false (we are justified to believe that $p$ is true because there's no obligation to refrain us from doing so), This sort of implication puts facts, and unfalsifiable assertions into one basket. We are able to be justified in believing any sort of assertion even if it is not justified.\cite{sep-epistemology}
                    \\
                    \\
                    As an easy example of how a dentological justification might result in a false belief let us review the Russell's teapot analogy, though the claim is to show that the philosophic burden of proof lies upon a person making empirically unfalsifiable, the example would also show how a dentological justification is weak. \cite{enwiki:1149010951}
                    \\
                    \\
                    In his paper "Is There a God?" we have:
                    \begin{qt}
                        Many orthodox people speak as though it were the business of sceptics to disprove received dogmas rather than of dogmatists to prove them. This is, of course, a mistake. If I were to suggest that between the Earth and Mars there is a china teapot revolving about the sun in an elliptical orbit, nobody would be able to disprove my assertion provided I were careful to add that the teapot is too small to be revealed even by our most powerful telescopes. But if I were to go on to say that, since my assertion cannot be disproved, it is intolerable presumption on the part of human reason to doubt it, I should rightly be thought to be talking nonsense. If, however, the existence of such a teapot were affirmed in ancient books, taught as the sacred truth every Sunday, and instilled into the minds of children at school, hesitation to believe in its existence would become a mark of eccentricity and entitle the doubter to the attentions of the psychiatrist in an 10 enlightened age or of the Inquisitor in an earlier time.
                    \end{qt}
                    Although the debate, that is there any evidence to prove gods existence remains, for what seems like forever, the burden of proof is always upon the one claiming it. There are several more cases that can easily fit under the label \textit{Knowledge}, that no-one would accept, most of the folklore sotries of beings such as Zeus, Thor, unicorns, etc... were widely regarded as a true belief, with the justification that you cannot disprove their existence. But any person in the 21st century would deny their existence. Therefore, the second type (Sufficiently Likely Justification), seems to work the best for most cases, and be the justification that Russell would mean.\cite{Russell1952}
                    \\
                    \\
                    \subsection{Kant, Schelling, Fichte and Evidence}  As we mentioned the idea of Parmendise was that the true knowledge must be based only upon rational thought, which lead to reconstruction of reality with deductive method. This has been applied by mathematician Euclid for the axiomatic foundations of his geometry, which was leading to the modern-rationalism. The idea was that the true knowledge should be deducible in its entirety from a highest,  and in itself irrefutable, but also to be capable of providing a justification for the claim that the knowledge deduced form it should be coherent and true. principle, which is still carried on by physicists who are dealing with the ultimate formulation that would describe the nature in its entirety. 
                    \\
                    \\
                    This is a very basic but logical step toward knowledge, since the world around seems to have order, and order seems to originate in logic (you may be able to make an orderly system with chaos at its foundations but we are trying to follow a more common step.).\cite{Kuppers2018-vv}
                    \\
                    \\
                    Kant's philosophy focuses on power and limitations of reson. Kant asks two questions whether reasoning can give us metaphysical knowledge, as rationalists claim? and whether reason can guide action and justify moral principles. He claims against the rationalists that if boundaries like knowledge of god or a world beyond senses is not considered, reasoning would provide contradiction. And for the empiricists, who claimed that emotions, and not reason would giude us toward act, he claims that reason can guide us toward principles that can be shared among rational beings.
                    \\
                    \\
                    Kant argues that we obtain knowldge by two ways: sensibility and unedrstanding. Empirical judgement depend on both. Later in the book he discusses the \textit{Transcendental Dialectic}. He argues against the efforst by philosophers such as Parmendise that try to subject the true knowledge free of worldly objects. He adds that the \textit{Dialectic} for the things, which are not revelaed by any senses is \textit{logic of illusion} (This turns out to be an important part in later chapters where we enter mathematical logic, and show that there can be multiple logical systems, however inconvenient.)\cite{sep-kant-reason}
                    \begin{qt}
                        The law of reason to seek unity is necessary, since without it we would have no reason, and without that, no coherent use of the understanding, and, lacking that, no sufficient mark of empirical truth
                    \end{qt}
                    The analysis of the conditions under which knowledge becomes possible leg him to the concept of \textit{Transcendental Subject} as the source of knowledge. prior toall experience. He argues that the subject's perception of the externam world is affected by, as he called it, \textit{Things in Themselves}. Following Kant, this constitute reality intrinsically, that is, independently of how we may experience it. This can be thought of as objectivism, putted more easily by Democritus, the idea that has been employed in Democritus' discussion of the gods, where he clears that out knowledge of the gods comes from giant films of atoms (since he constructed his worldview with atomistic view). A report credits that Democritus and Leucippus argue that thought as well as sensation are caused by images impinging on the body from outside, and that thought as much as perception depends on images.\cite{Cartledge1998-CARDTG-3}\cite{sep-democritus}\cite{Graham2010-th}
                    \\
                    \\
                    This conception was rejected first by Fichte, in 1794, in his \textit{"Doctrine of Science"}. He aegues that the knowledge engendering function of "things in themselves" leaves knowledge still dependent upon the external world, and that knowledge therefore lacks the property of being unconditional. However, in Fichte's viewm unconditionality is an indispensable prerequisite if knowledge acquired by the transcendental subject is to be absolute and no longer dependent upon changes in external experience. Fichte therefore set out from the idea that the actions of the transcendental subject must be completely unconditional, that it, caused only by itself.
                    \\
                    \\
                    The radical subjectivism that we encounter here was already in Fichte's time a target of criticism. Fichte's philosophical approach, promoting the perceiving subject to the soleand unconditioned source of knowledge, led inevitably to a contradiction with empirical reality. In respect of its understanding of reality, the subjective idealism of Fichte clearly reveals the same weakness, the same loss of reality as did Parmenides' doctrine of true being.
                    \\
                    \\
                    In his \textit{"Ideas for a Philosophy of Nature"}, published in 1797, Schelling attempted to correct this deficiency by first objectifying the subject-object identity and not as, Fichte had done, regarding it as an identity proceeding exclusively from the subject. Moreover, according to Schelling the subject-object identity must be considered as absolute. This means that the entire Subjective is at the same time the entire Objective, and the entire Objective is at the same time entire Subjective.
                    \\
                    \\
                    Unlike Fichte, in Schellings philosophy real world is more than just an image of the ideal world. He considered the conceptual and material appearances as two manifestations of the same entity, which was an absolute subject-object identity.
                \begin{qt}
                    “Nature is the visible mind, the mind is invisible Nature”. This is to be taken as meaning that the perceiving subject can regard itself in Nature as in a mirror. Nature is the visible mind. Conversely, mind is invisible Nature, insofar as mind mirrors Nature at the highest level of its being. Thus, mind in Nature and Nature in mind can contemplate one another.
                \end{qt}
                In Schelling's system, the task of empirical science is —at best— to verify the principles dictated to it by natural philosophy. On no account could they be disproved: the refutation of these principles would immediately have refuted the principles of reason and, thus, pursued the possibilities of cognition ad absurdum. \textit{In fact, the principles of natural philosophy were seen as unchallengeably certain. If empirical results did not accord with them, then the principles remained unchallenged, whereas the empirical observations were taken to be obviously at fault, or incomplete, or deceptive.} This is yet again closer to the concepts introduced by Parmendise and Fichte than to what is science trying to acheive. Although it is true that logical statements remain unchallenged, this is not because they have a higher values regarding empiricism. The logical statements remain unchallenged because they statements are build upon logical systems, which are agreed upon. In that sense any logical system would be working as good as science, and if so the nature would act in thousands of ways at the same time just to make the logic be true. But if we assume that the logic of the world remains a unique one, then working with logics and using empirical information just to prove our points seems to be missing an important fact, that we are to choose between many ways of possible logical system and yet expect that to be the right one, which describes the universe, by pure chance.
                \\
                \\
                A further aspect of Schelling's epistemology should be emphasized. In accordance with the identity principle, the ideal and the real together make up a whole that cannot be transcended. The whole is at the same time an allegory for the absolute, which however only reveals itself in the dichotomous form --that is, in ideal and real essence-- to the subject. However, the absolute, whe it “expands” into the ideal and the real, must not lead out of the absolute. As the absolute, it must always remain identical with itself in its entire absoluteness.
                \begin{qt}
                    Natural philosophy and empirical research into Nature are thus concerned with two fundamentally different objects of knowledge. One is concerned with “Nature as a subject” and the other with “Nature as an object”. “Nature as a subject” is a metaphor for the infinite productivity of Nature (“natura naturans”). It is downright natural dynamics. Its driving forces are the creatively acting natural principles, the discovery of which is the task of natural philosophy.  “Nature as an object”, in contrast, is the productivity of Nature as made manifest in her products (“natura naturata”). These products are in themselves finite and appear as a terminated network of actions, the elucidation of which is the task of empirical research into Nature. However, to avoid the conceptional separation of Nature into two forms, Schelling employed an artifice.
                \end{qt}
                According to this, the productivity of Nature is not really extinguished in its products; rather, it still remains active with a force of production that, however, is infinitely delayed. As already encountered in the philosophy of the Eleates, the concept of the infinite again must be invoked in order to save the consistency of the epistemological model.\cite{Kuppers2018-vv}
    
                In summary, we can say that Schelling's philosophy of Nature ran counter to today's scientific method in two important respects: 
                \begin{itemize}
                    \item Theory occupies a more important place than empiricism. Claims to truth need not stand the test of experience; they are exclusively derived from logical reasoning. In short: Knowledge a priori is given precedence over knowledge a posteriori.
                    \item The research strategy propagated by Descartes, Newton and others, according to which one should proceed from the simple to the complex, from the part to the whole, from the cause to the effect, is turned by Schelling into its opposite. The analytical method, based upon dismantling, abstraction and simplification, is discarded—or at least diminished in importance—in favour of a holistic method.
                \end{itemize}
                The addendum to the Introduction of Schelling's "Ideas of a Philosophy of Nature", in which he repeatedly attempts to express the inexpressible, is rich in morsels of poetic word-creation and pictorial comparison that exhaust themselves in nebulous abstraction. We read, for example, that the absolute is “enclosed and wrapped up into itself", or that the absolute “is born out of the night of its being into the day”. There Schelling speaks of the “æther of absolute ideality” and the "mystery of Nature".
                \\
                \\
                Even the closest philosophers of the Jena Romantics' Circle, such as Friedrich Schlegen and Johann Wilhelm Ritter, criticised the notion that pure speculation, unaided by any experience, could provide the basis for any profound knowledge about the world. We shall according to Ritter, : approach imperceptibly the true theory, without searching for it-- we shall find it by observing what really happens, for what more do we desire of the atheory than that it tells us what is really happening?"\cite{Kuppers2018-vv} 
    
            \section{Evidentialism} 
                Whether a blief is truley justified or not, there's something that makes it so. But before we begin with the concept of evidence it is useful to check some ideas from Parmendise, Schelling and others.\cite{sep-epistemology}
                \\
                \\
                The strongest case for a justification to be accurate seems to be the case of evidence.
                \begin{define}
                    According to Evidentialism, one is justified to believe something if and only if that person has evidence which supports said belief. \cite{enwiki:1149588226}
                \end{define}
                It is commonsense that one possessing a belief must have a reason to, whatever that may be. This dependence on reasons seems to be central to the very concept of justified only if one has adequate reasons to believe. Richard Feldman and Earl Conne, were two leading defenders of ecidentialism using their definition:
                \begin{define}
                    Evidentialism is a thesis about the justifactory status of all the doxastic attitudes: belief, disbelief, and suspension of judgement.
                \end{define}
                There fore defining justification all again as:
                \begin{define}
                    The Doxastic attitude, $d$, toward $p$ is justified for one at $t$ if and only if one's evidence at $t$ supports ones tading $d$ towards $p$.
                \end{define}
                \newpoint{Evidence:} Evidence for or against $p$ is any information that can evaluate the validity of $p$. That is to say any statement that can prove $p$ of being true or false.\cite{Mittag2015-qc}
                \\
                \\
                With such broad definition we can consider Schelling, Fichte, and Parmendise as evidence. But as we memtioned the nature of such justifications are far form being scientific. Here we examine ways that one would consider as ways  to acheive evidence
                \\
                \\
                \subsection{Rationalism and Reliabilism} We already investigated how Schelling would consider rational thought and logical process to be evidential. Though he considered empiricism to be important he didn't consider it as equal to ratoinal thought, therefore we can consider him as a rationalis. Rationalists believe that logic is the source of knowledge, Syllogisms, which is a logical argument that applies deductive reasoning to arrive at a conclusion, can be used --and if used properly-- to acheive knowledge. \cite{enwiki:1145200450}
                \begin{align*}
                    \text{If} A, \text{then} B;
                    \\ A;
                    \\ \therefore B
                \end{align*}
                The problem with such method is, that there's no limits on what permises you use. One can form such arguments using this logic:
                \begin{align*}
                    \text{If Alice is awake, it is morning.}
                    \\ \text {Alice is awake.}
                    \\ \therefore \text{It's morning.}
                \end{align*}
                Although many people would consider these statements as true and thus regard the whole argument as true, one can easily see that the premise would not necessarily follow the conclusion (Alice could grow insomnia and not sleep at all). Beside the usage of different logic systems that can raise contradiction with the reality, using premises that doesn't imply the conclusion are another problems many logical arguments can have. This can be at best put in a toolbox of evidences, not the evidence as a whole.
                \\
                \\
                Reason alone, shall not be taken for granted. The premises should follow the conclusion, and the logic must be the actual logic that is working in reality (or nature). The conclusion of this point is that rational thought is not enough to be considered evidence. \cite{CW/E}
                \\
                \\
                Extending beyond human reasoning which shown to be mistakeful, what if we use a mathematical approach. The investigation over if mathematics can be a solid ground work would be passed to next sections, but for now, as we mentioned for the logical errors one might get consider the following argument:
                \begin{align*}
                    x &=c \\
                    x^2 &= cx\\
                    x^2 - c^2 &= cx - c^2\\
                    (x+c)(x-c) &= c(x-c)\\
                    x + c &= c \\
                    2c &= c \\ 
                    2 &=1 \\ 
                \end{align*}
                Here we found an argument that works by mathematical laws but yet results in errors. To be more accurate this problem is not because of matheamtics, since in itself won't allow $(x-c)$ to be canceled out from the sides of the equation when it's zero. Here we made the mistake by not using the logical system properly. \cite{CW/E}
                \\
                \\
                \subsection{Coherentism}
                 We might look for an all-time true statement, which cannot be false anyway, and then try to work through other statments to see if they can be implied by that statement, or any other statemtens, which validity are proved by the original one. By this method we are looking for coherent statements beside the original-true-statement. 
                \\
                \\
                Coherentism lends itself to yet another way of knowinig that can be similarly flawed, the \textit{Perfect credibility}, In the paper \cite{CW/E}, Wenning argues:
                \begin{qt}
                    To the medieval mind it was only reasonable that the Earth was at the center of the universe, the lowest point possible under the heavens. To medieval thinkers humanity was at the center of the universe not because of our noble status as the pinnacle of creation, but because we were so very despicable with our fallen nature. Closer to the center of the universe still was that place at the very center of the Earth that was reserved for the most despicable of all – hell. Those not so terribly bad were relegated to the underworld or Hades upon death, but not hell.
                \end{qt}
                This is the reason why the medieval viewpoint envisioned heaven as “up” and hell as “down.” Man’s position near or at the center of the universe was not pride of place; rather, it was a matter of making perfect sense in man’s relationship with the deities. This belief was perfectly credible. Interpreting things in any other way would have made no sense given the then prevailing theological understanding.But still these conclusions were false, this problem arises because in choosing a coherent statement with the all-true-statement we are neglecting that in principle such statement may be false, thus the system would collapse as soon as one of the all-true-statements proved to be wrong. Using such method can be helpful to gather coherent statements together, but also can be dangerous if it is to lead us alone. We would like to find a law that implies good results and then finding coherent laws that would imply more things but we have to make sure that we are always in search to disprove such law. But how?
                
            \section{Empiricism}
                EMpiricists also endorse the Intuition/Deductio thesis as would rationalists do but for a significat difference, the use of rational though, mathematics and logic is not to prove all alone but to guide us throught what to check for.  By contrast, empiricists reject the Innate Knowledge and Innate Concept theses. Insofar as we have knowledge in a subject, our knowledge is gained, not only triggered, by our experiences, be they sensorial or reflective. Experience is, thus, our only source of ideas. Moreover, they reject the corresponding version of the Superiority of Reason thesis. Since reason alone does not give us any knowledge, it certainly does not give us superior knowledge. Empiricists need not reject the Indispensability of Reason thesis, but most of them do.\cite{sep-rationalism-empiricism}
                \begin{define}
                    \textit{The EMpiricism Thesis: } We have no source of knowledge in S or for the concepts we use in S other than experience.
                \end{define}
                \begin{callout}
                    To be clear, the Empiricism thesis does not entail that we have empirical knowledge. It entails that knowledge can only be gained, if at all, by experience.
                \end{callout}
                The Empiricism is not better just because of empirical data, but also since it uses the best other ways have to offer. The data is to prevent us from being mistakeful. 
                \\
                \\
                We are blinded by many things, our logical errors, our low intuitions in the world, our ability to cause mistakes in logical systems and so on, so one would argue that it is logical to find a source, not in ourselves but in anything else, that acts in logics, but also cannot be threatend by our flaws. The reason natural philosophy has took the turn into empiricism might be the from that reason, since the task itself is to describe nature, why not ask her for knwoledge.
                \\
                \\
                Here one might ask for the role rational thought, mathematics and logic. The use of them are not to invent the nature, as we saw earlier they can put us in a lot of trouble. But the problems with purely observing at nature is two, firstly it is pretty hard to dervie a conclusion since the systems in nature are complex, thus one cannot observe and the dervie the rule behind it, although we try to isolate simple system in the lab to be able to preceive the most from them. Second there are many ways to describe a natural process in terms of words and intuition,  Aristotle said that objects fall because each of the four elements (earth, air, fire, and water) had their natural place, and these elements had a tendency to move back toward their natural place. Thus, objects that were made of earth wanted to return to Earth, whereas fire, for example, rose toward heaven.\cite{greg-grav}. This would make us consider a more rigid form of reasoning, that is mathematics, which uses more than words, but describes relations:
                \begin{equation}
                    F = \frac{d}{dt} p
                \end{equation}
                The newton law describes the relation between a net force, and the objects momentum, which are also defined using mathematics. With this equation we don't need to say what an object if moving toward is, we just need to ask if there's a force acting upon it. Therefore it is necessary to use a rigid format of logic to describe what seems to be a logical nature.
        
            \section{Formal Language Theory}
                The investigation of Incompleteness theorem requiers the knowledge of some basic ideas such as languages, grammars, automatas and others. Therefore we start this section by talking about these topics in a general format. A more detailed version would be published later.\cite{Leary2019-ip}
                \subsection{Languages}
                    We would begin by the definition of a language. In a informal way a language is what we speak or what we write on a paper to give information to another person, mathematically and a more abstract idea of a language is a bunch of symbols that we have to make words (gluing symbols together to resemble a unity of them); To give a precise mathematical definition of language we would first define alphabet as:
                    \begin{define}
                        \textit{An alphabet $\Sigma$, is a set of symbols.}
                    \end{define}
                    \begin{define}
                        \textit{A word $M$, is a combination of symbols from alphabet $\Sigma$.}
                    \end{define}
                    For instance one could choose the set $\{a,b,c,d,\dots\}$ as ones alphabet. Then "$cat$" is a word in it's alphabet. As you can see there's a meaning for the word we gave as an example. It defines an object (more accurately a living creature). Since there can be Infinetly many words for any alphabet (other than an empty set which have no words), we distinguish between the accepted combinations and the ones we don't accept by defining a grammar for our language;  A grammar is a way to characterize a language, a way to list which stings of $\Sigma$ is acceptable. We could simply list strings or have a set of rules (or an algorithm) to say if a given combination is acceptable or not for a language. Thus we define a language as:
                    \begin{define}
                        \textit{Given an alphabet $\Sigma$, $\Sigma^\infty$ is the set of all possible words in the alphabet.}
                    \end{define}
                    \begin{define}
                        \textit{A subset $S$ of a set $X$ is decidable if and only if there exists a function that given $x\in X$ decides if $x\in S$ is true or false.}
                    \end{define}
                    \begin{define}
                        \textit{A Language $L$, is a subset of the alphabet $\Sigma^\infty$ ($L\subset \Sigma^\infty$) where there exists a function $\eta(\sigma\in\Sigma^\infty)$ called grammar that decides $L$.}
                    \end{define}
                    Formally, we define a grammar as:
                    \begin{define}
                        \textit{A Grammar is a set $\{V_T,V_N,S, R\}$ where $V_T$ is the set of terminal elements, $V_N$ is the set of non-terminal elements, $S$ is a memeber of $V_N$, and $R$ is a finite set of rules.}
                    \end{define}
                    We would use these definitions in the later sections of this paper. But for now let us give a formal definition of $R$ as well:\cite{Leary2019-ip}
                    \begin{define}
                        \textit{$R$ is a finite set of ordered pairs from $\Sigma^\infty V_N \Sigma^\infty\times \Sigma^\infty$, where $\Sigma = V_T\cup V_N$.}
                    \end{define}
                    \newpoint{Induction:} Induction is a method of proving that a statemens $P(n)$ is true for every natural number $n$, that is, that the infinitely many cases, $P(0), P(1),\dots$ all hold. \cite{enwiki:1157726892}
                    \begin{qt}
                        Mathematical induction proves that we can climb as high as we like on a ladder, by proving that we can climb onto the bottom rung (the basis) and that from each rung we can climb up to the next one (the step).
                    \end{qt}
                    \begin{theorem}
                        For every natural number $n$,
                        \begin{equation}
                            1+2+\dots + n = \frac{n(n+1)}{2}
                        \end{equation}
                    \
                    \proof If $n=1$, the equality holds. For the inductive case, fix $k\geq 1$ and assume that:
                    \begin{equation}
                        1+2+\dots+k =\frac{k(k+1)}{2}
                    \end{equation}
                    Now adding $k+1$ to each side we have:
                    \begin{equation}
                        1+2+\dots+(k+1) = \frac{k(k+1)}{2}+(k+1)
                    \end{equation}
                    Since the right hand side simplifies to:
                    \begin{equation}
                        \frac{(k+1)((k+1) + 1)}{2}
                    \end{equation}
                    Finishing the inductive step and thus the proof. As you can see in the inductive step what we prove is that:
                    \begin{qt}
                        \textit{If the formula holds for $k$, then the formula holds for $k+1$.}
                    \end{qt}
                    \end{theorem}
                    Looking at this from a slightly different angle, what we have done is to construct a set of numbers with a certain property. If we let $S$ stand for the set of numbers for which our theorem holds, in out proof by induction we whow that the set $S$, is identical with the set of natural numbers, thus the theorem holds for every natural number $n$, as needed.
                    \\
                    \\
                    So what makes a proof by induction work is the fact that the natural numbers can be defined recursively, There is a base case, consisting of the smallest natural number, and there is a recursive case, showing how to construct bigger natural number from smaller ones.
                    \\
                    \\
                    \newpoint{Terms and Formulas:} As we mentioned earlier not all words of a set $\Sigma^\infty$ is meaningful. Since any combination of the alphabet is a word there has to be distinctions between what are meaningful words and what are not. We would consider two kinds of words as \textit{terms \& formulas} as follow:
                    \begin{define}
                        \textit{If $\curveL$ is a language, a \textbf{term of $\curveL$} is a nonempty finite string $t$ of symbols from $\curveL$ such that either:}
                        \begin{enumerate}
                            \item $t$ is a variable, or
                            \item $t$ is a constant symbol, or 
                            \item $t:\equiv ft_1t_2t_3\dots t_n$, where $f$ is an $n$-ary function symbol of $\curveL$ and each of the $t_i$ is a term of $\curveL$.
                        \end{enumerate}
                    \end{define}
                    \begin{define}
                        \textit{If $\curveL$ is a language, a formula of $\curveL$ is a nonempty finite string of $\phi$ of symbols from $\curveL$ such that either:}
                        \begin{enumerate}
                            \item $\phi :\equiv = t_1t_2,$ where $t_1, t_2$ are terms of $\curveL$, or 
                            \item $\phi :\equiv R t_1t_2\dots t_n$ where $R$ is an $n$-ary relation symbol of $\curveL$ and $t_1, t_2, \dots , t_n$ are all terms of $\curveL$, or 
                            \item $\phi :\equiv (\neg \alpha)$ where $\alpha$ is a formula of $\curveL$, or
                            \item $\phi:\equiv (\alpha\lor \beta)$, where $\alpha$ and $\beta$ are formulas of $\curveL$, or 
                            \item $\phi :\equiv (\forall v)(\alpha)$, where $v$ is a variable and $\alpha$ is a formula of $\curveL$
                        \end{enumerate}
                    \end{define}
                    Notice that the five clauses of the definition can be separated into two groups. The first two clauses, the atomic formulas, are explicitly defined. The last three clauses are the recursive case, showing how if $\alpha$ and $\beta$ are formulas, they can be used to build more complex formulas, such as $(\alpha\lor\beta)$ or $(\forall v)(\alpha)$.
                    Now since the collection of formulas is defined recursively, we can use an inductive style proof when we want to prove that something is true about every formula. The inductive proof will consist of two parts, a base cae and an inductive case. First we prove the statement for every atomic formula and then using the inductive method we prove it for recursive formulas from the atomic ones. \textit{This method is called induction on the complexity of the formula, or induction on the structure of the formula.}
                    \\
                    \\
                    \newpoint{A First-order Language:} Before getting to Sentences one should know a definition for a first-order language. A first-order language $\curveL$ is defined as an infinite collection of symbols, separated into the following categories:
                    \begin{itemize}
                        \item \textit{Parentheses:} $(,)$.
                        \item \textit{Connectives:} $\land, \lor, \neg$.
                        \item \textit{Quantifier:} $\forall, \exists$.
                        \item \textit{Variables:} one for each positive integer $n$ denoted: $v_n$ for $n$th number.
                        \item \textit{Equality:} $=$.
                        \item \textit{Constant:} We can have a new symbol for each positive number or any other method that we distinguish between two numbert (we can use | for 1, || for 2 etc)
                        \item \textit{Functions:} For each positive integer $n$, some set of zero or more $n$-ary function symbols.
                        \item \textit{Relation:} For each positive integer $n$, some set of zero or more $n$-ary relation symbols.
                    \end{itemize}
                    \begin{callout}
                        Having $n$ arity means that it is intended to represent a function of $n$ variables.
                    \end{callout}
                    Notice that by defining such language one can avoid the process of finiding an algorithm of grammar (the one that differenciates between nonesense and meaningful words) since we defined all the possible functions and etc. This way we have to only

                    \newpoint{Sentences:} Among the formulas in the language $\curveL$, there are some in which we will be especially interested. These are the sentences of $\curveL$. The formulas that can be either true or false in a given mathematical model.\cite{Leary2019-ip}
                    \begin{define}
                        \textit{A sentence in a language $\curveL$ is a formula of $\curveL$ that contains no free variable.}
                    \end{define}
                    \newpoint{Structures: } Defining any language and with constansts, functions and etc. There can exist multiple ways to define structures as opposed to another. The point is that there is no preference. Thus without determining the structure under consideration, without deciding how we wishs to interpret the symbols of the language, we have no way of talking about the truth or falsity of a sentence. Thus we have:
                    \begin{define}
                        \textit{Fix a language $\curveL$. An $\curveL$-Structure $\curveA$ is a nonempty set $A$m caled the \textbf{Universe of $\curveA$}, together with:}
                        \begin{enumerate}
                            \item \textit{For each constant symbol $c$ of $\curveL$, an element $c^\curveA$ of $A$}
                            \item \textit{For each $n$-ary function symbol $f$ of $\curveL$, a function $f^\curveA L A^n\rightarrow A$, and }
                            \item \textit{For each $n$-ary relation symbol $R$ of $\curveL$, and $n$-ary relation $R^\curveA$ on $A$.}
                        \end{enumerate}
                    \end{define}
                    \newpoint{Truth in a Structure:} Now that we know some formal rules about what constitutes a language, we would like to merge syntax and semantics. We want to answer what is means to say that an $\curveL$-formula is true in an $\curveL$-structure $\curveA$.
                    \\
                    \\
                    To begin the process of tying together the symbols with the structures, we will introduce assignment functions. These assignment functions will formalize what it means to interpret a term or a formula in a structure.
                    \begin{define}
                        \textit{if $\curveA$ us an $\curveL$-structure, a \textbf{variable assignment function into $\curveA$} is a function $s$ that assigns to each variable an element of the universe $A$. So a variable assignment function into $\curveA$ is any function with domain $V$ and codomain $A$.}
                    \end{define}
                    We will have occasion to want to fix the value of the assignment function $s$ for certain variables, then:
                    \begin{define}
                        \textit{If $s$ is a variable assignment function into $\curveA$ and $x$ is a variable and $a\in A$, then $s[x|a]$ is the variable assignment function into $\curveA$ defined as follows:}
                        \begin{equation}
                            s[x|a](v) = \left\{
                                \begin{matrix}
                                    s(v) & \text{if} \ v \ \text{is a variable other than }x\\
                                    a & \text{if} \ v \ \text{is the variable} x 
                                \end{matrix}\right.
                        \end{equation}
                    \end{define} 
                    We call the function $s[x|a]$ and $x$-modification of the assignment function $s$. This is essentially the same function, except that the variable $x$ is now assigned to a particular element of the universe.
                    \\
                    \\
                    What we will do next is extend a variable assignment function to a term assignment function $\bar s$. Thius function will assign an element of the universe to each term of the language $\curveL$.
                    \begin{define}
                        \textit{Suppose that $\curveA$ is an $\curveL$-structure and $s$ is a variable assignment function into $\curveA$. The function $\bar s$ is called the term assignment function generated by $s$, is the function with domain consisting of the set of 
                        $\curveL$-terms and codomain $A$ defined recursively as follows:}
                        \begin{enumerate}
                            \item \textit{If $t$ is a variable, $\bar s(t) = s(t)$}.
                            \item \textit{If $t$ is a constant symbol $c$, then $\bar s(t)= c^\curveA$}.
                            \item \textit{If $t:\equiv ft_1t_2\dots t_n$, then $\bar s(t) = f^\curveA (\bar s(t_1),\bar s(t_2),\dots,\bar s(t_n))$}.
                        \end{enumerate}
                    \end{define}
                    Although we will be primarily interested in truth of sentences, we will
                    first describe truth (or satisfaction) for arbitrary formulas, relative to an
                    assignment function.
                    \begin{define}
                        \textit{Suppose that $\curveA$ us an $\curveL$-structure, $\phi$ is an $\curveL$-formula, and $s$: $V\rightarrow A$ is an assignment function. We will say that $\curveA$ satisfies $\phi$ with assignment $s$, and write $\curveA\vDash \phi[s]$, in the following circumstances:}
                        \begin{enumerate}
                            \item \textit{If $\phi :\equiv =t_1t_2$ and $\bar s(t_1)$ is the same element of the universe $A$ as $\bar s(t_2)$, or}
                            \item \textit{If $\phi :\equiv Rt_1\dots t_n$ and $(\bar s(t_1),\dots,\bar s(t_n))\in R^\curveA$, or}
                            \item \textit{If $\phi:\equiv (\neg \alpha)$ and $\curveA \not\vDash \alpha[s]$ (where $\not\vdash$ means "does not satisfy") or}
                            \item \textit{If $\phi:\equiv (\alpha\lor\beta)$ and $\curveA\vDash\alpha[s]$, or $\curveA \vDash\beta[s]$ (or both), or}
                            \item \textit{$\phi:\equiv(\forall x)(\alpha)$ and, for each element $a$ of $A$, $\curveA\vDash \alpha[s(x|a)]$}
                        \end{enumerate}
                    \end{define}
                    \begin{callout}
                        If $\Gamma$ is a set of $\curveL$-formulas, we say that $\curveA$ satisfies $\Gamma$ with assignment $s$, and write $\curveA\vDash \Gamma[s]$ if for each $\gamma\in\Gamma, \curveA\vDash \gamma[s]$
                    \end{callout}
                    \newpoint{Substitutions and Substitutability:} Suppose that you knew the sentence $\forall x \phi(x)$ was tru in particular structure $\curveA$. Then, if $c$ is a constant symbol in the language, you would certainly expect $\phi(c)$ to be true in $\curveA$ as well.
                    \\
                    \\
                    Suppose now that $\curveA\vDash \forall x\exists y\neg(x=y)$. This sentence is, in fact, true in any structure $\curveA$ such that $A$ has atleast two elements. The rules of substitutability that we will discuss in this section are designed to help us avoid this problem, the problem of attempting to substitute a term inside a quantifier that binds a variable involved in the term.
                    \begin{define}
                        \textit{Suppose that $u$ is a term, $x$ is a variable, and $t$ is a term. We define the term $u_t^x$, and read $u$ with $x$ replaced by $t$ as:}
                        \begin{enumerate}
                            \item \textit{If $u$ is a variable not equal to x, then $u_t^x$ is $u$}
                            \item \textit{If $u$ is $x$, then $u_t^x$ is $t$}
                            \item \textit{If $u$ is a constant symbol then $u_t^x$ is $u$}
                            \item \textit{If $u:\equiv fu_1u_2\dots u_n$, where $f$ is a $n$-ary function symbol and the $u_i$ are terms, then:}
                            \begin{equation}
                                u_t^x \ \text{is} \ f(u_1)_t^x\dots(u_n)_t^x
                            \end{equation}
                        \end{enumerate}
                    \end{define}
                    Having defined what it means to substitute a term for a variable now we define what is substitutability:
                    \begin{define}
                        \textit{Suppose that $\phi$ is a $\curveL$-formula, $t$ is a term, and $x$ is a variable. We say that $t$ is substitutable for $x$ in $\phi$ if:}
                        \begin{enumerate}
                            \item \textit{$\phi$ is atomic, or}
                            \item \textit{$\phi :\equiv \neg(\alpha)$ and $t$ is substitutable for $x$ in $\alpha$, or}
                            \item \textit{$\phi:\equiv (\alpha\lor\beta)$ and $t$ is substitutable for $x$ in both $\alpha$ and $\beta$}
                            \item \textit{$\phi:\equiv (\forall y)(\alpha)$ and either}
                            \begin{enumerate}
                                \item \textit{$x$ is not free in $\phi$, or}
                                \item \textit{$y$ does not occur in $t$ and$t$ is substitable or $x$ in $\alpha$.}
                            \end{enumerate}
                        \end{enumerate}
                    \end{define}
                    \newpoint{Logical Implication: } In this section we formalize the question that If I know this statement is true, is it necessarily the case that this other statement is true.
                    \begin{define}
                        \textit{Suppose that $\Delta$ and $\Gamma$ are sets of $\curveL$-formulas. We will say that $\Delta$ logically implies $\Gamma$ and write $\Delta\vDash \Gamma$ if for every $\curveL$-structure $\curveA$, if $\curveA\vDash\Delta$, then $\curveA\vDash\Gamma$.}
                    \end{define}
                    This definition says that if $\Delta$ is true in $\curveA$, then $\Gamma$ is true in $\curveA$. Remember, for $\Delta$ to be true in $\curveA$, it must be the case that $\curveA\vDash\Delta[s]$ for every assignment function $s$.
                    \begin{define}
                        An $\curveL$-formula $\phi$ is said to be valid if $\emptyset\vDash\phi$, in other words, if $\phi$ is true in every $\curveL$-structure with every assignment function $s$. In this case we will write $\vDash\phi$.
                    \end{define}
                    \begin{callout}
                        For the double turnstyle symbol $\vDash$, if there is a structure on the left, $\curveA\vDash \sigma$, we are discussing truth in a single structure. On the other hand if there is a set of sentences on the left $\Gamma\vDash\sigma$, then we are discussing logical implication.
                    \end{callout}
                \subsection{Deductions}
                    In this section we will try to formalize what is known to be the deductive method. In mathematics we generally tend to insist upon the existence of a proof for any true statement (or at least we hope so). A proof is a sequence of statements, each one of which can be justified by referring to previous statements. This is a perfectly reasonable starting point, and it brings us to the main difficuly we will have to address as we move from an informal understanding of what constitutes a proof to a formal definition of deduction.
                    \\
                    \\
                    The proofs that you have seen in your mathematical career have had a couple of nice properties. The first of these is that proofs are easy to follow. This doesn't mean that it is easy to discover a proof, but rather that if someone is showing you a proof, it should be easy to follow the steps of the proof and to understand why the proof is correct. The second admirable property of proofs is that when you prove something, you know that it is true! Our definition of deduction will be designed to make sure that deductions, too, will be easily checkable and will preserve truth.
                    \\
                    \\
                    We then would impose the following restrictions on our logical axioms and rules of inference:
                    \begin{enumerate}
                        \item There will be an algorithm that will decide, given a formula, whether or not that formula is a logical axiom.
                        \item There will be an algorithm that will decide, given a finite set of formulas $\Gamma$ and a fimula $\theta$, whether or not $(\Gamma,\theta)$ is a rule of inference.
                        \item For each rule of inference $(\Gamma,\theta)$, $\Gamma$ will be a finite set of formulas.
                        \item Each logical axiom will be valid.
                        \item Our rules of inference will preserve truth. In other words, for each rule of inference $(\Gamma, \theta)\rightarrow \Gamma\vDash\theta$.
                    \end{enumerate}
                    The idea is that there should be no brilliance and no insight required to check whether an alleged deduction of $\alpha$ is, infact, a deduction of $\alpha$. To check whether a deduction is correct will be such a simple procedure that it could be programmed into a computer.
                    \\
                    \\
                    We begin by fixing a language $\curveL$. Also assume that we have been given a fixed set of $\curveL$-formulas, $\Lambda$, is called the set of logical axioms, and a set of ordered pairs $(\Gamma, \phi)$, called the rules of inference. A deduction is going to be a finite sequence or ordered list, of $\curveL$-formulas with certain properties.
                    \begin{define}
                        \textit{Suppose that $\Sigma$ is a collection of $\curveL$-formulas and $D$ is a finite sequence $(\phi_1, \dots, \phi_n)$ of $\curveL$-formulas. We will say that $D$ is a deduction form $\Sigma$ if for each element in $D$:}
                        \begin{enumerate}
                            \item $\phi_i\in\Lambda$ ($\phi_i$ is a logical axiom), or
                            \item $\phi_i\in\Sigma$ ($\phi_i$ is a nonlogical axiom), or
                            \item There is a rule of inference $(\Gamma,\phi_i)$ such that $\Gamma\subseteq\{\phi_1,\dots,\phi_{i-1}\}$
                        \end{enumerate}
                    \end{define}
                        If there is a deduction form $\Sigma$, the last line of which is the formula $\phi$, we will call this a deduction form $\Sigma$ of $\phi$. And write: $\Sigma\vdash\phi$.
                        \begin{callout}
                            Well, we have now established what we mean by the word justified. In a deduction we are allowed to write down any $\curveL$-formula that we like, as long as that formula is either a logical axiom or is listed explicitly in a collection $\Sigma$ of nonlogical axioms. Any formula that we write in a deduction that is not an axiom must arise from previous formulas in the deduction via a rule of inference.
                        \end{callout}
                        \subsubsection{Logical Axioms} 
                            In this section we will gather together a collection of $\Lambda$ of logical axioms for $\curveL$. This set of axioms, though infinite, will be decidable. Which means that there exists an algorithm, which given an input $x$, can tell if $x\in\Lambda$ is true or false.
                            \\
                            \\
                            \newpoint{Equality Axioms:} We have taken the route of assuming that the equality symbol, $=$, is a part of the language iteself. There are three groups of axioms that are designed for this symbol. The first just says that any object is equal to itself:
                            \begin{equation}
                                x = x
                            \end{equation}
                            For the second group of axioms, assume that $x_i$ and $y_i$ are variables, and $f$ is an $n$-ary frunction symbol.
                            \begin{equation}
                                \left[(x_1 = y_1)\land (x_2 = y_2) \land\dots\land(x_n = y_n) \right] \rightarrow \left(f(x_1,x_2,\dots,x_n) = f(y_1,y_2,\dots,y_n)\right)
                            \end{equation}
                            The assumption for the third group of axioms is the same as for the second group, except that $R$ is assumed to be an $n$-ary relation symbol.
                            \begin{equation}
                                \left[(x_1 = y_1)\land (x_2 = y_2) \land\dots\land(x_n = y_n) \right] \rightarrow \left(R(x_1,x_2,\dots,x_n) = R(y_1,y_2,\dots,y_n)\right)
                            \end{equation}
                            \newpoint{Quantifier Axioms:} The quantifier axioms are designed to allow a very reasonable sort of entry in a deduction. Suppose that we know $\forall xP(x)$. Then, if $t$ is any term of the language, we should be able to state that $P(t)$ . To avoid some problems we will demand that the term $t$ be substitutable for the variable $x$.
                            \begin{align}
                                (\forall x\phi)\rightarrow \phi_t^x, \text{if } t \text{ is substitutable for } x \text{ in } \ \phi \\
                                \phi_t^x\rightarrow (\exists x\phi), \text{if } t \text{ is substitutable for } x \text{ in } \ \phi
                            \end{align}
                            In many logic texts, the first axiom would be called universal instantiation, while the second would be known as existential generalization. 
                        \subsubsection{Rules of Inference:}
                            There will be two types of Rules of Inference, one dealing with propositional consequence and one dealing with quantifier.
                            \\
                            \\
                            \newpoint{Propositional Consequence:}We work with a restricted language $curveP$, consisting only of a set of propositional variables $A,B,C,\dots$ and the connectives $\lor$ and $\neg$ . Notice there are no quantifiers, no relation symbols, no function symbols, and no constansts. Each propositional variable can be assigned one of two truth values, T,F for truth and falsity respectively. The we can define a function $v$ to assign the truth value and for extending that we would define:
                            \begin{equation}
                                \bar v(\phi) = \left\{
                                \begin{matrix}
                                    v(\phi) &\text{if } \phi \text{ is a propositional variable}
                                    \\
                                    F & \text{if } \phi:\equiv(\neg\alpha) \text{ and } \bar v{\alpha} = T
                                    \\
                                    F & \text{if } \phi:\equiv(\alpha\lor\beta) \text{and } \bar v(\alpha) = \bar (\beta) = F
                                    \\
                                    T & \text{otherwise}
                                \end{matrix}
                                \right.
                            \end{equation}
                            To discuss propositional consequence in first-order logic, we will transfer our formulas to the real of propositional logic and use the idea of tautology in that area. To be specific, given $\beta$ , an $\curveL$-formula of first-order logic, gere is a procedure that will convert $\beta$ to a formula $\beta_P$ of propositional logic corresponding to $\beta$:
                            \begin{enumerate}
                                \item Find all subfomulas of $\beta$ of the form $\forall x\alpha$ that are not in the scope of another quantifier. Replace them with propositional variables in a systematic fashion. This means that if $\forall yQ(y,c)$ appears twice in $\beta$ it is replaced by the same letter both times, and distinct subformulas are replaced with distict letters.
                                \item Find all aromic formulas that remain, and replace them systemically with new propositional variables.
                                \item At this point, $\beta$ will have been replaced with a propositional formula $\beta_P$
                            \end{enumerate}
                            We are now almost at a point where we can state out propositional rule of inference. Recall that a rule of inference is an ordered pair $(\Gamma,\phi)$, where $\Gamma$ is a set of $\curveL$-formulas and $\phi$ is a $\curveL$-formula.
                            \begin{define}
                                \textit{Suppose that $\Gamma_P$ is a set of propositional formulas and $\phi_P$ is a propositional formula. We will say that $\phi_P$ is a propositional consequence of $\Gamma_P$ if every truth assignment that makes each propositional formula in $\Gamma_P$ true also makes $\phi_P$ true. Notice that $\phi_P$ is a tautology if and only if $\phi_P$ is a propositional consequence of $\emptyset$.}
                            \end{define}
                            Notice that if $\Gamma_P=\{\gamma_{1P},\dots,\gamma_{nP}\}$ is a nonempty finite set of propositional formulas and $\phi_P$ is a propositional formula, then $\phi_P$ is a propositional consequence of $\Gamma_P$ if and only if:
                            \begin{equation}
                                \left[
                                    \gamma_{1P}\land\dots\land \gamma_{nP} 
                                \right]\rightarrow \phi_P 
                            \end{equation}
                            is a tautology.
                            \begin{callout}
                                A tautology is a formula of assertion that is true in every possible structure $\curveA$.
                            \end{callout}
                            Now we extend our definition:
                            \begin{define}
                                Suppose that $\Gamma$ is a finite set of $\curveL$-formulas and $\phi$ is an $\curveL$-formula. We will say that $\phi$ is a propositional consequence of $\Gamma$ if $\phi_P$ is a propositional consequence of $\Gamma_P$, where the two latter are the results of applying the procedure of making a first order logic formulas into propositional formulas.
                            \end{define}
                            \begin{define}
                                If $\Gamma$ is a finite set of $\curveL$-formulas, $\phi$ is an $\curveL$-formula, and $\phi$ is a propositional consequence of $\Gamma$ then $(\Gamma,\phi)$ is a rule of inference of type (PC).
                            \end{define}
                            \newpoint{Quantifier Rules} Suppose, without making any particular assumptions about $x$, that you were able to prove $x$ is $a$ then you also have proved $(\forall x)x$ is $a$. Looking at it from back to front we would have:
                            \begin{define}
                                Suppose that the variable $x$ is not free in the formula $\psi$. Then both of the following are rules of inference of type (QR).
                                \begin{align}
                                    \left(\left\{
                                        \psi\rightarrow\phi
                                    \right\}, \psi \rightarrow (\forall x \phi)\right)\\
                                    \left(\left\{
                                        \phi\rightarrow\psi
                                    \right\}, \exists x\phi \rightarrow \psi\right)
                                \end{align}
                            \end{define}
                \subsection{Soundness}
                    Generally in mathematics we would like to be sure that when something has been proved, then it is for sure, true. In thi section we will prove a theorem that shows that the logical system that we have developed has this highly desirable property. This result is called the Soundness Theorem. As you might recall the requirements that we set for our rule of inference are:
                    \begin{enumerate}
                        \item There will be an algorithm that will decide, given a formula $\theta$, whether or not $\theta$ is a logical axiom.
                        \item There will be an algorithm that will decide, given a finite set of formulas $\Gamma$ and a formula $\theta$, whether or not $(\Gamma, \theta)$ is a rule of inference.
                        \item For each rule of inference $(\Gamma,\theta)$, $\Gamma$ will be a finite set of formulas.
                        \item Each logical axiom will be valid.
                        \item Our rules of inference will preserve truth, or in other words, for each rule of inference $(\Gamma,\theta)$, $\Gamma\vDash \theta$.
                    \end{enumerate}
                    These requirements serve two purposes: They allow us to verify mechanically that an alleged deduction is in fact a deduction, and they provide the basis of the Soundness Theorem. The idea behind the Soundness theorem is very simple. Suppose that $\Sigma$ is a set of $\curveL$-formulas and suppose that there is a deduction of $\phi$ from $\Sigma$. What the Soundness Theorem tells us is that in any structure $\curveA$ that makes all of the formulas of $\Sigma$ true, $\phi$ is true as well.
                    \begin{theorem}
                        If $\Sigma\vdash\phi$, then $\Sigma\vDash\phi$
                        \\
                        \\
                        \textit{proof.} Let $\thm_\Sigma = \{\phi | \Sigma \vdash\phi\}$ and $C =\{\phi | \Sigma \vDash \phi\}$. By showing that $\thm\subseteq C$ we prove the theorem. Notice that $C$ has the following characteristics:
                        \begin{enumerate}
                            \item $\Sigma \subseteq C$. If $\sigma\in\Sigma$ then certainly $\Sigma\vDash\sigma$.
                            \item $\Lambda\subseteq C$. As the logical axioms are valid, they are true in any structure. Thus $\Sigma\vDash\lambda$ for any logical axiom.
                            \item If $(\Gamma,\theta)$ is a rule of inference and $\Gamma\subseteq C$, then $\theta\in C$. Then because: $\curveA\vDash\Gamma$ and $\Gamma\vDash\theta$ we know that $\curveA\vDash\theta$.
                        \end{enumerate}
                        Since we have the following proposition and $C$ has these characteritics the prove is completed:
                        \textit{Fix sets of $\curveL$-formulas $\Sigma$ and $\Lambda$ and a collection of rules of inference. The set $\thm_\Sigma$ is the smallest set $C$ such:}
                        \begin{itemize}
                            \item $\Sigma\subseteq C$
                            \item $\Lambda\subseteq C$ 
                            \item If $(\Gamma,\theta)$ is a rule of inference and $\Gamma\subseteq C$, then $\theta\in C$.
                        \end{itemize}
                    \end{theorem}
            \section{Completeness}
                Before beginning with the incompleteness theorem it might help to know actually what is completeness. We have established a deductive system consisting of logical axioms and rules of inference. The Soundness Theorem showed that our deductive system preserves truth; that is, if there is a deduction of a formula $\varphi$ from a set of formulas $\Sigma$, then $\varphi$ is true in any model of $\Sigma$. Formally, we write this as follows:
                $$\Sigma \vdash \varphi \implies \Sigma \models \varphi$$
                where $\vdash$ denotes provability in the deductive system, and $\models$ denotes semantic entailment.
                
                The Completeness Theorem is the first major result of this chapter, and it gives us the converse to the Soundness Theorem. Specifically, it states that every semantically valid formula is provable in our deductive system. In other words, if a formula is true in every model, then it can be proved using only the logical axioms and rules of inference we have established. Formally, we write this as follows:

                \begin{equation} 
                    \Sigma \models \varphi \implies \Sigma \vdash \varphi
                \end{equation}
                
                Combined with the Soundness Theorem, the Completeness Theorem gives us the following equivalence:
                
                \begin{define}
                    A deductive system consisting of a collection of logical axioms $\Lambda$ and a collection of rules of inference is said to be complete if for every set of nonlogical axioms $\Sigma$ and every $\curveL$-formula $\phi$,
                    \begin{equation}
                        \text{If } \Sigma\vDash\phi \text{, then } \Sigma\vdash\phi
                    \end{equation}
                \end{define}

                This equivalence assures us that if our deductive system allows us to prove a statement, then that statement is true, and conversely, if a statement is true in every model, then it can be proved using our deductive system.
                
                However, it is important to note that the Completeness Theorem only applies to formulas that are true in every possible model. It does not necessarily extend to formulas that are true in some but not all models. Furthermore, the theorem applies specifically to first-order logic and may not hold for other logical systems.
            \section{Incompleteness Theorems}
                \subsection{Mathematics}
                    \subsubsection{Prespective}
                        In the last section we talked about the completeness theorem, which investigated the fact that our deductive system is sound and complete. For the collection of lpgical axioms that we have set out, any formula $\phi$ that can be deduced from a set of nonlogical axioms $\Sigma$ will be true in all models of $\Sigma$ under any variable assignment function (soundness), and furthermore any formula $\phi$ that is true in all models of $\Sigma$ under every assignment functionwill be deducible from $\Sigma$ (completeness)--This means that we can prove every true statement within the system. Thus our deductive system is the best it can be.\cite{Hajek2007-fq}
                        \\
                        \\
                        Now in this section we would try to show how would Incompleteness be proved, since we have shown that th efirst order logic is sound and complete it is trvial to infere that Incompleteness and Completeness are to be proved for different languages, and wether or not we can use them in Physics is to be worked out separately. But in the last sections we surely developed necessary concepts for one, who would like to talk about completeness (or incompleteness) of a physical theory.
                        \\
                        \\
                        Later on we would dicuss why in uor view. This might be the case.
                    \subsubsection{Basic Proof from the Theory of Algorithm\cite{Uspensky1995-sm}}
                        We assume that we are given a subset $T$ of language $L$, which is called the set of \textit{true statements}. This set should contain only the statements in the language that would evaluate as True. In the process of assuming such subset we ommit the part where we would consider the statemens true or false. A language then would be completely defined (for our purpose) if we are given the \textit{fundamental pair:}
                        \begin{define}
                            \textit{Given a language $L$ and the subset of true statements $T$, we call $\left<L,T\right>$ a fundamental pair.}
                        \end{define}
                        \begin{define}
                            \textit{\textbf{Unprovable} means not provable, and provable means there exist a proof for such statement.}
                        \end{define}
                        Also the concept of proof, deduction, and induction were defined in previous sections. For the sake of simplicity consider a deductive system that consists of an alphabet called \textit{Alphabet of Proofs}, and denoted $P$. Then we would generally have $P^\infty$ as all the words that alphabet can have, but restrictions by grammar, rules of inference, induction, deduction etc, would provide us with a set of proofs called $\bar P$. As you would recall our deductive system would prove a statement, and only one for each proof. Therefore given a language $\curveL_x$ and a proof language $P$ one would require the existence of an algorithm that can map a proof from $P$ to a true statement from $\curveL_x$. Therefore a deductive system (in the most simple manner, but you can still consider the deduction introduced in the previous sections.) would be made of $P$, $\bar P$, $\delta$, where $\delta$ is a function that maps proofs from $P$ to true statements from $\curveL_x$.
                        \begin{define}
                            \textit{We would call $\left<P,\bar P, \delta\right> a deductive system.$}
                        \end{define}
                        So our final definition is as follows:
                        \begin{enumerate}
                            \item We have a language $\curveL$ and an alphabet $P$ for the proofs. 
                            \item In the set $P^\infty$ we are given a subset $\bar P$, whose elements are called proofs. We futher assume:
                            \begin{enumerate}
                                \item We shall allow different concepts of proofs, different proof subsets of $P^\infty$. We shall also allow the alphabet $P$ to vary.
                                \item There has to be an effective method or algorithm to check if a given word in $P$ is a proof and to show what stetement it proves.
                            \end{enumerate}
                            \item We have a function $\delta$ (to determine what is being proved) whose domain of definition $\Delta$ satisfies $\bar P\subseteq \Delta\subseteq P^\infty$, and whose range of valurs is in $L^\infty$. We assue that we have an algorithm which computes this function. Therefore we might say that the proof $p$ in $\bar P$ is the proof of $\delta(p)$ in the language $\curveL$.
                        \end{enumerate}
                        Therefore we can now define consistency and completeness more easily:
                        \begin{define}
                            \textit{We say the deductive system $\left<P,\bar P, \delta\right>$ is consistent relative to the fundamental pair $\left<\curveL,T\right>$ if we have $\delta(\bar P) \subseteq T$. In other words our deductive system would not prove contradictory statements.}
                        \end{define}
                        \begin{define}
                            \textit{We say the deductive system $\left<P,\bar P, \delta\right>$ is complete relative to the fundamental pair $\left<\curveL,T\right>$ if we have $T \subseteq \delta(\bar P)$. In other words our deductive system would prove every true statements within the language.}
                        \end{define}
                        Conditions for the nonexistence of a complete and consistent deductive system can easily be given in terms of the theory of algorithms. An algorithm is a set of instructions which, given an input enables us to obtain an output if such an output exists or else obtain nothing at all if theres no output for out particular input. For our purpose consider that every input and output is a word thus there exists alphabet $I$ for inputs and alphabet $O$ for outputs.
                        \\
                        \\
                        In other words, in order to work with algorithms which process paris or sequences of words we must first compress such pairs or sequences of as single words in the algorithm, in a new alphabet.
                        \\
                        \\
                        Let $\star$ denote for a new letter not in our alphabet $\curveL$. Thus $\curveL_\star$ is a new alphabet. We would use such letter to denote separation between words in our alphabet.
                        \\
                        \\
                        Now we must define some terms:
                        \begin{define}
                            \textit{\textbf{Domain of Applicability:} The set of all inputa that can be processes by a given algorithm is called the domain of applicability of the algorithm.}
                        \end{define}
                        \begin{define}
                            \textit{The algorithm $A$ computes the function $f$, if we have $A(x)\sim f(x)$ for all $x$}
                        \end{define}
                        \begin{callout}
                            Note that $\sim$ is the conditional equality sign, both sides are equal when they are defined and bot sides are undefined under same $x$.
                        \end{callout}
                        \begin{define}
                            \textit{A function that can be computed by some algorithm is called a computable function.}
                        \end{define}
                        \begin{define}
                            \textit{\textbf{Decidable:} A subset $S$ of a set $A$ is said to be decidable if there exsits and algorithm which determines whether or not an element of $A$ is in $S$.}
                        \end{define}
                        We would require the set of all proofs be decidable set of all the words in proof alphabet. 
                        \begin{lemma}
                            For any subset $X$, the set $\emptyset$ and $X$ are decidable relative to $X$.
                            \proof We provide an algorithm for the subset $X$ of $X$ that always returnes true. And for the $\emptyset$ always false.
                        \end{lemma}
                        \begin{theorem}
                            If $T$ is an enumerable set, then one can find a complete and consistent deductive system for the fundamental pair $\left<\curveL, T\right>$
                            \proof if $T=\emptyset$ the deductive system $\left<P, \bar P, \delta\right>$ where $\bar P = \emptyset$ and $delta(p)=\emptyset$ is consistent and complete relative to $T$.
                            \\
                            Otherwise if $T \not = \emptyset$ since it is enumerable (there exists an algorithm that maps the natural numbers to the set.), we would have the algoritm $\tau$ that enumerates $T$. We would later prove that the set of all proofs is enumerable therefore by choosing $\delta = \tau$ we would have a consistent and complete deductive system.
                        \end{theorem}
                        \begin{lemma}
                            A decidable subset of an enumerable set is enumerable.
                            \proof if $f$ is the enumerating function of a set $A$. If $S\subseteq A$ is empty set. Then it is enumerable by definition. If not there exists $s\in S$ thus we enumerate the set by the following function:
                            \begin{equation}
                                g(n)=\left\{\begin{matrix}
                                    f(n) & \text{if } f(n) \in S\\
                                    s & \text{otherwise}\end{matrix}\right.
                            \end{equation}
                        \end{lemma}
                        \begin{lemma}
                            A subset $S$ of an enumerable set $X$ is decidable relative to $X$ is decidable relative to $X$ if and only if both $S$ and its complement $X\backslash S$ are enumerable.
                            \proof If $S$ is decidable, then so is $X\backslash S$, and so both $S$ and $X\backslash S$ are enumerable by Lemma 2. Conversely, suppose that both $S$ and $X\backslash S$ are enumerable. If either one is empty then, $S$ is decidable, by Lemma 1. Suppose both $S$ and its complement are nonempty sets, in which case they are enumerated by some computable functions $f$ and $g$, respectively. Then, for deciding $x$ we need only compute $f(0),g(0),f(1),g(1),\dots$ until we encounter $x$.
                        \end{lemma}
                        \begin{theorem}
                            The set of all proofs is enumerable.
                            \proof Fir any enumerable alphabet $L$ we have:
                            \begin{equation}
                                L^\infty = \left\{x | x\in L\right\} \cup \left\{xy|x,y\in L\right\}\dots
                            \end{equation}
                            Therefore we have an algorithm that goes through all the words now since the alphabet of $P$ is enumerable, then $P^\infty$ and its subsets are enumerable.
                        \end{theorem}
                        \begin{lemma}
                            Suppose that $R$ is an enumerable set and $f$ is a computable function which is defined on all elements of $R$. Then $f(R)$ is an enumerable set.
                            \proof If $R$ is empty, then so is $f(R)$ therefore enumerable by definition. If $R$ is enumerated by a function $\rho(x)$ then $f(R)$ is enumerated by $\eta(x) = f(\rho(x))$.
                        \end{lemma}
                        \begin{theorem}
                            The set of all provable words (for a deductive system) is enumerable.
                            \proof Let $Q$ be the set of all provable words for the deductive system $\left<P, \bar P, \delta\right>$  obviously $Q = \delta(\bar P)$. However $P$ is enumerable by theorem 4. Hence $Q$ is also enumerable by lemma 4.
                        \end{theorem}
                        It follows now that if $T$ is not enumerable set, then it is impossible to find a complete and consistent deductive system for the pair $\left< \curveL,T\right>$, since the set $Q$ of provable words for any consistent deductive system is a proper subset of $T$, and there will have to be an element on complement $T\backslash Q$. Such an element is a true but unprovable statement!
                        \\
                        \\
                        Theorems 3 and 5 together give a condition on a fundamental pair which is necessary and sufficient for the existence of a complete and consistent deductive system for the pair. This condition is enumerability of the set of all truths. 
                        \\
                        \\
                        One would expect that in a "rich" or "expressive" language the set of all truths is too complicated to be enumerable, and hence there are no complete and consistent deductive systems for such a language. However, the criterion in theorems 3 and 5 is not very convineint to use, since it is often difficult to study the entire set $T$. 


                \subsection{Philosophy}
                        In this section we would try to understand the incompleteness theorems out of the formal mathematical language. We would then proceed to talk about a mathematical universe.
                        \subsection{Introduction}
                            Gödel's incompleteness theorems are groundbreaking outcomes in present-day logic that transformed the comprehension of mathematics and logic, and had significant implications for the philosophy of mathematics. While some philosophers have attempted to utilize these outcomes in other areas of philosophy, the validity of many such applications is often debatable.
                            \\
                            \\
                            To comprehend Gödel's theorems, it is imperative to clarify crucial ideas related to them, including "formal system," "consistency," and "completeness." Essentially, a formal system signifies a collection of axioms furnished with inference rules, which enable the generation of new theorems. The axioms must be finite or at least decidable, meaning that an algorithm can determine whether a given statement is an axiom or not. If this criteria is fulfilled, the theory is known as "recursively axiomatizable," or simply "axiomatizable." The inference rules of a formal system are also effective operations that allow one to ascertain if one has made a legitimate application of a rule of inference. As a result, it is always possible to determine if any given finite sequence of formulas constitutes a real derivation in the system, given the axioms and the rules of inference.
                            \\
                            \\
                            A formal system is complete if every statement of the language of the system can be derived (proven) in the system or its negation can be proven. A formal system is consistent when there exists no statement in which the statement itself and its negation can both be derived in the system. In this context, only consistent systems are of interest since an inconsistent formal system renders every statement derivable, hence rendering such a system trivially complete.
                            \\
                            \\
                            Gödel discovered two distinct yet interconnected incompleteness theorems, typically known as the first and second incompleteness theorems. Although "Gödel's theorem" may refer to both in conjunction, it often refers to the first one separately. With a modification introduced by J. Barkley Rosser in 1936, the first theorem can be roughly stated as follows:
                            \begin{qt}
                                Any consistent formal system $F$ within which a certain amount of elementary arithmetic can be carried out is incomplete, meaning there are statements in the language of $F$ which can neither be proved nor disproved in $F$.
                            \end{qt}
                            Gödel's theorem not only asserts the existence of such statements, but also explicitly produces a specific sentence that is neither provable nor refutable in formal system $F$ through Gödel's proof method. This "undecidable" statement can be mechanically generated from a specification of $F$ and is a relatively simple statement of number theory, a purely universal arithmetical sentence.
                            \\
                            \\
                            One common misconception about Gödel's first theorem is interpreting it as showing the presence of truths that cannot be proven. However, this is incorrect because the incompleteness theorem deals with derivability in a particular formal system, rather than provability in an absolute sense. For any statement $A$ unprovable in a given formal system $F$, there are other formal systems where $A$ is provable (by taking $A$ as an axiom). On the other hand, the standard axiom system of Zermelo-Fraenkel set theory (denoted as $ZF$ or $ZFC$ with the axiom of choice) is powerful enough to derive all ordinary mathematics. Nevertheless, there exist arithmetical truths that are not provable even in $ZFC$ due to Gödel's first theorem. Hence, proving them would require a formal system that goes beyond $ZFC$ methods. Therefore, these truths are not provable using today's "ordinary" mathematical methods and axioms, nor can they be proved in a way that mathematicians would regard as conclusive and unproblematic.\cite{Hajek2007-fq}
                            \\
                            \\
                            Gödel's second incompleteness theorem concerns the limitations of consistency proofs and can be roughly stated as:
                            \begin{qt}
                                For any consistent system $F$ within which a certain amount of elementary arithmetic can be carried out, the consistency of $F$ cannot be proved in $F$  itself.
                            \end{qt}
                            The second theorem requires a slightly more elaborate version of formal system $F$, encompassing more arithmetic than the one required for the first theorem, which holds under much weaker assumptions. It is important to note that, like the first incompleteness theorem, the second theorem pertains only to formal provability or derivability relative to some formal system (in this case, $F$ itself). It does not imply anything about whether the statement "$T$ is consistent" can be proven true by an argument considered conclusive or acceptable by mathematicians for a particular theory $T$ satisfying the conditions of the theorem. For many theories, such proof is possible.
                            
                        \subsection{Incompleteness Theorems and The Church-Turing Thesis}
                            Initially, Gödel demonstrated the incompleteness of a particular but extensive formalized theory $P$, which is a variation of Russell's type-theoretical system $PM$ (Principia Mathematica). This system encompasses all extensions of $P$ with the same language whose set of axioms is primitive recursive. Although Gödel did not demonstrate, he suggested that the proof could be adapted to apply to standard axiom systems of set theory such as $ZFC$. At that time, it was unclear how general the result was, despite the fact that Gödel had a very general outcome.
                            \\
                            \\
                            What remained uncertain was the analysis of the intuitive notion of decidability, necessary in characterizing the concept of an arbitrary formal system. Mathematicians and logicians have employed the intuitive notion of a decision method since ancient times. For the general results such as the general incompleteness theorems or the undecidability results, however, a precise mathematical explication of the notion would be necessary. Instead of decidable sets or properties, effective or computable functions or operations are often considered, but these are two sides of the same coin.
                            \\
                            \\
                            Gödel, Church, and Turing independently proposed different exact mathematical definitions of computable functions and decidable sets of numbers, which were later found to be equivalent. Turing's careful conceptual analysis that used fictional and abstract computing machines, conventionally known as "Turing machines," was significant, as emphasized by Gödel himself. The intuitive notion and some of these mathematical explications are often labeled as "The Church-Turing thesis." For historical reasons, the term "recursive function" has been dominant in the logical literature, so decidable sets are often termed as "recursive sets."
                            \\
                            \\
                            To comprehend the incompleteness and undecidability results correctly, understanding the distinction between two key concepts regarding sets is crucial. First, there may be a mechanical method that decides whether any given number belongs to the set or not, in which case, the set is referred to as "decidable" or "recursive." Second, there may be a mechanical method that generates or lists the elements of the set, number by number. In the latter case, the set is referred to as "recursively enumerable" (r.e.), meaning it can be effectively generated or is "semi-decidable." A fundamental outcome of the theory of computability is that semi-decidable sets can be effectively generated but not decidable. This is essentially the essence of the first incompleteness theorem at an abstract level. However, if both a set and its complement are recursively enumerable, the set is recursive, i.e., decidable.\cite{sep-church-turing}
            \section{Nature aa a Mathematical Entity}
                The incompleteness theorem, together with the computation theorem (Church-Turing Thesis) establish a limit on mathematical abstraction. Being depended on the choice of axioms the two refer to unprovable or incomputable entities in mathematics and computations. There can be a lot to learn from these two theorems and to grasp a better understanding of the world as a whole. But there is a subtle difference between a physicists and a mathematicians. 
                \\
                \\
                The assumption that world is the realism of an abstraction is not a new one. In fact, it is as ancient as Plato! 
                \subsection{Platonism in Mathematics}
                    \subsubsection{What is Mathematical Platonism}
                        Mathematical platonism can be defined as the conjunction of the following three theses:
                        \begin{enumerate}
                            \item  \textbf{Existence: }There are mathematical objects.
                            \item  \textbf{Abstractness:} Mathematical objects are abstract.
                            \item  \textbf{Independence:} Mathematical objects are independent of intelligent agents and their language, thought, and practices.
                        \end{enumerate}
                        Platonism, not just limited to mathematics, encompasses any belief system that is based on the three aforementioned claims. The first two claims are relatively straightforward: existence can be expressed as "$\exists xMx$," where "$Mx$" represents the predicate "$x$ is a mathematical object," which refers to objects studied in pure mathematics such as numbers, sets, and functions. Abstractness asserts that all mathematical objects are non-spatiotemporal and therefore causally inefficacious. \cite{sep-platonism-mathematics}
                        \\
                        \\
                        Independence, however, is a bit more complex. It is commonly understood as the notion that even if there were no intelligent agents or if their language, thought, or practices were different, mathematical objects would still exist. However, this interpretation may be insufficient to fully capture the concept of independence At present, the concept of independence remains somewhat vague.
                    \subsubsection{Historical Remarks}
                        Platonism should be distinguished from the historical view of Plato himself. In contemporary debates about platonism, few argue for a strong exegetical interpretation of Plato's view, let alone defend it. While the term "platonism" is inspired by Plato's theory of abstract and eternal Forms, the current definition of platonism is independent of its original historical inspiration.\cite{sep-platonism-mathematics}
                        \\
                        \\
                        It's important to note that platonism is purely a metaphysical view and distinct from other views with substantive epistemological content. Past characterizations of platonism often included strong epistemological claims that implied we have an immediate insight or grasp of the realm of abstract objects. However, it is now more common to reserve the term "platonism" for the purely metaphysical view described earlier. Many philosophers who defend platonism in this sense would reject the additional epistemological claims, including Quine and those drawn to the indispensability argument, which aims to provide an empirical defense of mathematical platonism.
                        \\
                        \\
                        Lastly, the definition of "mathematical platonism" excludes the claim that all truths of pure mathematics are necessary, despite it being traditionally held by most platonists. This exclusion is justified by the fact that some philosophers considered platonists, such as Quine and advocates of the aforementioned indispensability argument, reject this additional modal claim.
                    \subsubsection{The Philosophical Significance of Mathematical Platonism}
                        Mathematical platonism has significant philosophical implications that challenge the physicalist idea that reality is limited to the physical world. According to platonism, reality extends beyond the physical and includes abstract objects that are not part of the spatiotemporal and causal order studied by the physical sciences. Additionally, if mathematical platonism is true, it would put pressure on many naturalistic theories of knowledge because we undeniably possess mathematical knowledge, which implies knowledge of causally inefficacious objects.
                        \\
                        \\
                        While these philosophical consequences are not exclusive to mathematical platonism, this form of platonism is uniquely equipped to support them due to the tremendous success of mathematics as a discipline in its own right and as a tool for other sciences. Contemporary analytic philosophers are reluctant to contradict any of the core claims of a discipline with the scientific credentials of mathematics. Rejecting mathematics outright would be unattractive if philosophical analysis were to reveal some strange and surprising consequences of the discipline. In contrast, a form of platonism based on a discipline with less impressive scientific credentials than mathematics would not be as fortunate. For example, when theology leads to unexpected philosophical consequences, many philosophers do not hesitate to reject the relevant parts of theology.
                    \subsubsection{Object Realism}
                        Object realism asserts the existence of abstract mathematical objects and is the combination of Existence and Abstractness. This view stands in contrast to nominalism, which holds that there are no abstract objects, although the term "nominalism" may refer to the belief that there are no universals in traditional philosophical usage. 
                        \\
                        \\
                        Object realism is logically weaker than mathematical platonism because it does not include Independence. Consequently, the philosophical implications of object realism are not as strong as those of platonism. Physicalists may accept non-physical objects if they are dependent on or reducible to physical objects, such as corporations, laws, and poems. Furthermore, there is no mystery about acquiring knowledge of non-physical objects that we have constituted or made.
                        \\
                        \\
                        Some views in the philosophy of mathematics align with object realism but are not platonist. Traditional intuitionist views, for example, affirm the existence of mathematical objects but maintain that these objects depend on or are constituted by mathematicians and their activities. 
                    \subsubsection{Truth-Value Realism}
                        Truth-value realism posits that every well-formed mathematical statement has a unique and objective truth-value that is independent of our ability to know it or whether it logically follows from current mathematical theories. Furthermore, this view asserts that most statements deemed to be true in mathematics are indeed true. Although this is a metaphysical view, it differs from platonism in that truth-value realism does not commit to an ontology of mathematical objects to explain the truth-values of mathematical statements.
                        \\
                        \\
                        Platonism may provide motivation for truth-value realism by explaining how mathematical statements acquire their truth-values. However, truth-value realism does not entail platonism or object realism unless additional premises are added. Nominalists, who do not believe in the existence of abstract objects, may still endorse truth-value realism, at least for basic branches of mathematics such as arithmetic. In this case, they translate mathematical statements into a language that does not rely on the existence of abstract objects.
                        \\
                        \\
                        Some philosophers suggest that the debate about platonism should be transformed into a debate about truth-value realism because it is clearer and more important to both philosophy and mathematics than the former. They argue that the debate about truth-value realism is more tractable than the debate about platonism, which is often unclear.
                    \subsubsection{The Mathematical Significance of Platonism}
                        Working realism is a methodological view that advocates for practicing mathematics as if platonism were true, according to Bernays (1935) and Shapiro (1997, pp. 21-27 and 38-44). This requires further clarification. Platonism has been used in the past to defend particular mathematical methods in debates about the foundations of mathematics, including:
                        \begin{enumerate}
                            \item Classical first-order (or stronger) languages whose singular terms and quantifiers appear to be referring to and ranging over mathematical objects. (This contrasts with the languages that dominated earlier in the history of mathematics, which relied more heavily on constructive and modal vocabulary.)
                            \item Classical rather than intuitionistic logic.
                            \item Non-constructive methods (such as non-constructive existence proofs) and non-constructive axioms (such as the Axiom of Choice).
                            \item Impredicative definitions (that is, definitions that quantify over a totality to which the object being defined would belong).
                            \item ‘Hilbertian optimism’, that is, the belief that every mathematical problem is in principle solvable.
                        \end{enumerate}
                        Working realism and platonism are two distinct views with regards to mathematics. While working realism is a view within mathematics that concerns the correct methodology of the discipline, platonism is an explicitly philosophical view. Working realism does not take a stance on whether classical methods require any philosophical defense, or if such defense must be based on platonism.
                        \\
                        \\
                        However, there are logical relations between the two views. If mathematical platonism is true, then it would justify the classical methods associated with working realism. Mathematical platonism would also encourage the search for new axioms to settle questions left open by current mathematical theories, such as the Continuum Hypothesis.
                        \\
                        \\
                        On the other hand, working realism does not necessarily imply platonism. While working realism says we are justified in using the platonistic language of contemporary mathematics, this falls short of platonism in at least two ways. Firstly, the platonistic language of mathematics can be analyzed in a way that avoids reference to and quantification over mathematical objects. Secondly, even if a face-value analysis of the language of mathematics could be justified, this would support object realism but not platonism. An additional argument would be needed for the third component of platonism, namely, Independence.\cite{sep-platonism-mathematics}\cite{Benson2006-jm}\cite{Tegmark2008-qv}
                \subsection{The Fregean Argument for Existence}
                    Here, we present a general template for an argument that supports the existence of mathematical objects, which is commonly referred to as the Fregean argument. While Frege was the first philosopher to develop an argument of this form, the template can be applied more broadly and abstractly, without necessarily relying on specific aspects of Frege's own defense of the existence of mathematical objects.
                    \\
                    \\
                    It is important to note that Fregean logicism represents just one way in which this argument can be developed. Other philosophers have presented variations of this argument, which will be discussed below.
                    \subsubsection{Structure of the Argument}
                        The Fregran argument is based on two premises, the first of which concenrs the semantics of the language of mathematics:
                        \begin{qt}
                            \textbf{Classical Semantics}
                            \\
                            The singular terms of the language of mathematics purport to refer to mathematical objects, and its first-order quantifiers purport to range over such objects.
                        \end{qt}
                        The word 'purpot' needs to be explained. When a sentence $S$ purports to refer or quantify in a certain way, this means that for $S$ to be true, $S$ must succeed in referring or quantifying in this way. The second premise does not require much explanation:
                        \begin{qt}
                            \textbf{Truth}
                            \\
                            Most sentences accepted as mathematical theorems are true (regardless of their syntactic and semantic structure).
                        \end{qt}
                        Consider sentences that are accepted as mathematical theorems and that contain one or more mathematical singular terms. By Truth, most of these sentences are true. Let $S$ be one such sentence. By Classical Semantics, the truth of $S$ requires that its singular terms succeed in referring to mathematical objects. Hence there must be mathematical objects, as asserted by Existence.
                    \subsubsection{Defending Classical Semantics}
                        Classical Semantics is a hermeneutic claim about the language of mathematics, which asserts that it functions semantically in much the same way as ordinary language. This claim is descriptive rather than normative and is broadly compatible with traditional views on semantics. The plausibility of Classical Semantics stems from the apparent semantic similarities between mathematical and non-mathematical language, which are supported by standard semantic analyses proposed by linguists and semanticists.
                        \\
                        \\
                        However, some philosophers, including nominalists like Hellman and Hofweber, have challenged Classical Semantics. They argue that the semantic structure of mathematical language is fundamentally different from that of ordinary language and that this difference undermines the prima facie plausibility of Classical Semantics. To substantiate these challenges, they must show that the apparent semantic similarities between mathematical and non-mathematical language are deceptive and provide convincing arguments that linguists and semanticists would recognize as significant.
                    \subsubsection{Defending Truth}
                        There are a variety of ways to defend the truth of mathematical statements. In general, these defenses first identify a standard by which truth-values can be assessed and then argue that mathematical theorems meet this standard.
                        \\
                        \\
                        One option is to appeal to a fundamental standard outside of mathematics, such as in the case of Logicism. This approach claims that any theorem of pure logic is true and attempts to prove that theorems of certain branches of mathematics can be derived from pure logic and definitions.
                        \\
                        \\
                        Another option is to appeal to the standards of empirical science, as in the Quine-Putnam indispensability argument. Here, it's argued that any indispensable part of empirical science is likely to be true and therefore justifiable for belief. It's then claimed that large amounts of mathematics are indispensable to empirical science, and thus belief in Truth is justified.
                        \\
                        \\
                        A third option, referred to as naturalism or mathematical naturalism, appeals to the standards of mathematics itself. The justification for mathematical claims is based on the sui generis standards provided by mathematics, rather than appealing to non-mathematical standards like logic or empirical science. This strategy has gained attention recently. For instance, mathematicians generally accept the truth of mathematical theorems, and this acceptance constitutes a plausible stance for defending mathematical truth.
                        \begin{enumerate}
                            \item Mathematicians are justified in accepting the theorems of mathematics.
                            \item Accepting a mathematical statement $S$ invloves $S$ to be true.
                            \item When a mathematician accepts a mathematical statement $S$, the content of this attitude is in general the literal meaning of $S$.
                        \end{enumerate}
                        From these three claims it follows that mathematical experts are justified in taking the theorems of mathematics to be literal truths. By extension the rest of us too are justified in believing Truth. Note that the experts with whom (1) is concerned need not themselves believe (2) and (3), let alone be justified in any such belief. What matters is that the three claims are true. The task of establishing the truth of (2) and (3) may fall to linguists, psychologists, sociologists, or philosophers, but certainly not to mathematicians themselves.
                        \\
                        \\
                        Admittedly, fictionalists about mathematics will try to resist (2) or (3).\cite{sep-platonism-mathematics}
                    \subsubsection{The Notion of Ontological Commitment}
                        Versions of the Fregean argument are sometimes stated in terms of the notion of ontological commitment. Suppose we operate with the standard Quinean criterion of ontological commitment:
                        \begin{qt}
                            \textbf{Quine’s Criterion.}
                            A first-order sentence (or collection of such sentences) is ontologically committed to such objects as must be assumed to be in the range of the variables for the sentence (or collection of sentences) to be true.
                        \end{qt}
                        Classical Semantics holds that many mathematical sentences are committed to mathematical objects. This is because singular terms and first-order quantifiers, which are expressions that refer to or range over mathematical objects, must succeed in doing what they purport to do for a mathematical theorem to be true. Therefore, the theorem must be ontologically committed to mathematical objects according to Quine's Criterion. However, there have been challenges to this criterion, with some philosophers suggesting that not all of the objects within the range of the quantifiers need to exist for a sentence to be true, or that the link between the first-order existential quantifier and ontological commitment should be severed. 
                        \\
                        \\
                        One counterargument to these challenges is that the Fregean argument presented above does not rely on the concept of ontological commitment, and therefore, the challenges to its definition are irrelevant. However, challengers may argue that the argument's conclusion, Existence, is too weak to have its intended effect. The conclusion is formalized as '$\exists xMx$' in the philosophical meta-language $LP$, which may fail to incur ontological commitment if the challengers dispute the standard Quinean notion of ontological commitment. Ultimately, the challengers must provide an account of why their non-standard notion of ontological commitment is superior and more theoretically interesting than the standard Quinean notion.
                    \subsubsection{From Existence to Mathematical Platonism?}
                        Suppose we accept Existence, perhaps based on the Fregean argument. As we have seen, this is not yet to accept mathematical platonism, which is the result of adding to Existence the two further claims Abstractness and Independence. Are these two further claims defensible?
                        \\
                        \\
                        By the standards of philosophy, Abstractness has remained relatively uncontroversial. Among the few philosophers to have challenged it are Maddy (1990) (concerning impure sets) and Bigelow (1988) (concerning sets and various kinds of numbers). This relative lack of controversy means that few explicit defenses of Abstractness have been developed. But it is not hard to see how such a defense might go. Here is one idea. It is a plausible prima facie constraint on any philosophical interpretation of mathematical practice that it should avoid ascribing to mathematics any features that would render actual mathematical practice misguided or inadequate. This constraint makes it hard to deny that the objects of pure mathematics are abstract. For if these objects had spatiotemporal locations, then actual mathematical practice would be misguided and inadequate, since pure mathematicians ought then to take an interest in the locations of their objects, just as zoologists take an interest in the locations of animals. The fact that pure mathematicians take no interest in this question suggests that their objects are abstract.
                        \\
                        \\
                        Independence says that mathematical objects, if there are any, are independent of intelligent agents and their language, thought, and practices.
        \section{Determinism and Computation}
            \subsection{Turing Machines}
                Alan Turing introduced Turing machines in 1936-37 as basic theoretical computational devices designed to explore the boundaries of what can be computed and the limitations thereof. Originally referred to as "automatic machines," they were initially designed for computing real numbers and later dubbed "Turing machines" by Alonzo Church in his review of Turing's paper (Church 1937). Today, Turing machines are one of the fundamental models in computer science and computability theory that have significant implications for understanding the nature of computation and the limits of what computers can do.
                \\
                \\
                Turing introduced Turing machines in the context of research into the foundations of mathematics. More particularly, he used these abstract devices to prove that there is no effective general method or procedure to solve, calculate or compute every instance of the following problem:
                \begin{qt}
                    \textbf{Entscheidungsproblem} The problem to decide for every statement in first-order logic whether or not it is derivable in that logic.\cite{sep-spacetime-bebecome}
                \end{qt}
                The original version of the problem, as proposed by Hilbert and Ackermann in 1928, focused on the concept of validity rather than derivability. However, Gödel's completeness theorem from 1929 demonstrated that proving the existence (or non-existence) of an effective procedure for derivability would also resolve the issue of validity. To address this question properly, a clear understanding of what constitutes an "effective procedure" is necessary, which is where Turing machines come into play. These machines were developed by Alan Turing in 1936 as an abstract model of computation, providing a formalized definition of an effective procedure and allowing researchers to investigate the Entscheidungsproblem with greater precision. Ultimately, Turing's work established that there is no algorithmic solution for determining the provability of arbitrary statements, leading to the undecidability of the Entscheidungsproblem.
                \\
                \\
                A Turing machine, or computing machine as it was referred to by Turing, originally defined a machine that is capable of a finite set of configurations $q_1,\dots,q_n$ (referred to as m-configurations by Turing). The machine consists of a one-way infinite tape divided into squares, with each square able to hold exactly one symbol. These symbols are denoted by $S_0, S_1, ..., S_m$, where $S_1 = 0$ and $S_2 = 1$. During operation, the machine scans the content of one square on the tape, which can hold either a blank symbol ($S_0$) or one of the other symbols.
                \\
                \\
                The Turing machine is classified as an automatic machine, or a-machine, which means that its behavior at any given moment is entirely determined by the current configuration being scanned (which consists of the current state and symbol). This condition is known as determinacy. A-machines are distinguished from choice machines, which rely on the decision of an external operator or device to determine the next state (as described by Turing in 1936-7: 232). The Turing machine is capable of performing three types of action:
                \begin{itemize}
                    \item Print $S_i$ move one square to the left ($L$) and go to state $q_j$.
                    \item Print $S_i$, move one square to the right ($R$) and go to state $q_j$.
                    \item Print $S_i$, do not move ($N$) and go to state $q_j$
                \end{itemize}
                The ‘program’ of a Turing machine can then be written as a finite set of quintuples of the form:
                \begin{equation}
                    q_i S_jS_{i,j}M_{i,j}q_{i,j}
                \end{equation}
                Alan Turing's original definition of computing machines used two types of symbols - figures consisting entirely of 0s and 1s, as well as "symbols of the second kind" which were distinguished on the tape using a system of alternating squares. The sequence computed by the machine was contained in F-squares, while E-squares were used to mark F-squares and "assist the memory." However, using F and E-squares led to complications as noted by Emil L. Post.
                \\
                \\
                There are two important aspects to note regarding the Turing machine setup. Firstly, the machine's tape is potentially infinite, implying that the memory of the machine is also potentially infinite. Secondly, a function is considered Turing computable if there exists a set of instructions resulting in a Turing machine computing the function, regardless of how much time it takes. These assumptions ensure that no computable functions fail to be Turing-computable due to limited time or memory. Nonetheless, some Turing-computable functions may not be carried out by existing computers due to memory limitations, and others may never be computable in practice because they require more memory than can be built using all of the atoms in the universe. When a function is shown to be not Turing-computable, this result is very strong because it implies that no computer we could ever build would carry out the computation.\cite{sep-turing-machine}
            \subsubsection{Turing's Universal Machine}
                The universal Turing machine was developed to demonstrate the uncomputability of certain problems. Essentially, it is a Turing machine that can compute anything another Turing machine computes. Assuming that the Turing machine concept fully captures computability (which implies the validity of Turing's thesis), it follows that anything that can be computed can be computed by the universal machine. Conversely, any problem that cannot be computed by the universal machine is considered uncomputable.
                \\
                \\
                This is the powerful rhetorical and theoretical idea behind the universal machine - one simple formal device capable of capturing all possible processes involved in computing a number. It is also a key reason why Alan Turing is now revered as one of the founding fathers of computer science.\cite{sep-turing-machine}
            \subsection{Philosophical Issues Related to Turing Machines}             
                \subsubsection{Human and Machine Computations}
                Alan Turing's original identification between computable numbers and Turing machines aimed to demonstrate that the Entscheidungsproblem is not a computable problem and therefore not a "general process" problem. However, the question remains as to what Turing's intuitive notion of computability was and whether it actually encompasses all computable problems and types of computation. This is a fundamental issue in the philosophy of computer science.
                \\
                \\
                It should be noted that when Turing wrote his paper, modern computers had not yet been developed. Thus, interpretations that identify Turing computability with computability by a modern computer are not historically accurate statements of Turing's thesis. The computing machines of the time, such as differential analyzers and desk calculators, were limited in their computational abilities and were used primarily in the context of human computational practices. Therefore, Turing formalized human computation rather than machine computation in his paper, and his concept of computable problems referred to those that could be solved by human means.
                \\
                \\
                In Section 9 of his paper, Turing explained that Turing machines were a natural model of human computation by analyzing the process of human computation. He created an abstract human "computor" who fulfilled a set of conditions based on human limitations that restrict what we can compute (including limitations of our sensory and mental apparatus). This "computor" computed real numbers on an infinite one-dimensional tape divided into squares, with certain restrictions. These limitations and restrictions have been further analyzed by scholars such as Gandy and Sieg.
                \begin{itemize}
                    \item \textbf{Determinacy condition D} “The behaviour of the computer at any moment is determined by the symbols which he is observing and his ‘state of mind’ at that moment.” (Turing 1936–7: 250)
                    \item \textbf{Boundedness condition B1} “there is a bound B to the number of symbols or squares which the computer can observe at one moment. If he wishes to observe more, he must use successive observations.” (Turing 1936–7: 250)
                    \item \textbf{Boundedness condition B2} “the number of states of mind which need be taken into account is finite”
                    \item \textbf{Locality condition L1} “We may […] assume that the squares whose symbols are changed are always ‘observed’ squares.” (Turing 1936–7: 250)
                    \item \textbf{Locality condition L2} “each of the new observed squares is within L squares of an immediately previously observed square.” (Turing 1936–7: 250)
                \end{itemize}
                Turing's analysis and the resulting model are considered by many as the best standard model of computability today, owing to their so-called "direct appeal to intuition." This is because the conditions presented above can quite easily be used to derive Turing machines by analyzing them into simple operations that are elementary enough to not require further division. For a strong statement of this point of view, see Soare 1996.
                \\
                \\
                It is worth noting that although Turing's analysis of computation focused on human computation, his identification between (human) computation and Turing machine computation applied to the Entscheidungsproblem implies that he did not contemplate the existence of a model of computation that surpasses human computation capabilities, which can offer a general and effective method for solving the Entscheidungsproblem. If he had considered the possibility of such a model, he would have deemed the Entscheidungsproblem computable rather than uncomputable.
                \\
                \\
                Turing's focus on human computation in his analysis of computation has prompted researchers to broaden the scope of Turing's analysis to include computation by physical devices, leading to versions of the physical Church-Turing thesis. Robin Gandy, for instance, extended Turing's analysis to discrete mechanical devices and developed a new model based on a fundamental set of constraints that were reducible to the Turing machine model. Wilfried Sieg continued this work and proposed the framework of Computable Dynamical Systems (Sieg 2008). Others have explored the possibility of "reasonable" models from physics that can "compute" something non-Turing computable. For instance, Aaronson, Bavarian, \& Gueltrini 2016 (Other Internet Resources) demonstrated that the halting problem would be solvable with finite resources if closed timelike curves existed. Some have also suggested alternative models for computation that are inspired by the Turing machine model but better equipped to capture specific aspects of current computing practices. One example is persistent Turing machines designed to capture interactive processes. However, these proposals do not demonstrate the existence of "computable" problems beyond those that the Turing machine can compute. Some authors consider them reasonable models of computation that seemingly compute more than Turing machines, leading to debates in the computer science community on hypercomputation in the early 2000s. See Teuscher 2004 for various positions.\cite{sep-turing-machine}
        \subsection{Causal Determinism}
            I will generally use the term "determinism" throughout this piece, rather than "causal determinism," in line with recent philosophical practice of distinguishing views and theories on causation from conclusions about the success or failure of determinism. However, it is important to note that cause and effect are often closely related to our understanding of determinism, as we will explore later. Historically, determinism has been defined in various imprecise ways, which can be problematic when investigating determinism within a specific theoretical context. To avoid major errors in definition, we can start with a loose but comprehensive definition of determinism:
            \begin{qt}
                Determinism: The world is governed by (or is under the sway of) determinism if and only if, given a specified way things are at a time $t$, the way things go thereafter is fixed as a matter of natural law.
            \end{qt}
            The idea of determinism can be traced back to the philosophical notion that everything can, in principle, be explained - that everything has a sufficient reason for being and existing as it does. Leibniz termed this idea the Principle of Sufficient Reason. However, with the emergence of precise physical theories that appeared to be deterministic, the concept of determinism became distinguishable from these philosophical roots. In philosophy of science, there is frequent interest in determining the determinism or indeterminism of different theories, without necessarily starting from a view on Leibniz’ Principle.\cite{sep-determinism-causal}
            \subsubsection{Natural Laws}
                Throughout the scientific approach to the nature, we described the world in terms of states and the evolutions that they are undertaking. As Heraclitus wrote:
                \begin{qt}
                    Everything flows and nothing abides; everything gives way and nothing stays fixed.
                \end{qt}
                It was later on when we started to define these becomings in terms of mathematical equations. In classical mechanics, the state of a system can be fully determined by specifying its position and momentum at a particular moment in time, and its subsequent evolution is governed by deterministic differential equations. 
                \\
                \\
                However, in quantum mechanics, although the Schrödinger equation describing a system's wave function is deterministic, the relationship between the wave function and the observable properties of the system appears to be non-deterministic. The wave function provides all the information that can be known about the system, but it does not give a complete description of its properties. Instead, the wave function gives probabilities for different outcomes of measurements that can be performed on the system. This apparent non-determinism is a prominent characteristic of quantum mechanics that sets it apart from classical mechanics.\cite{enwiki:1092966327}
    \section{Is Quantum Mechanics Deterministic?}
        Now the formal and philosophical structure that we would need to have has been completed to a good degree. It's time to have a talk with quantum mechanics.
        \\
        \\
        The theory was developed almost in three years, a really quick way to have a fundamental theory of nature (I didn't consider the field theory or potentially the unified theory). The result that quantum mechanics had gave scientist was extremely accurate, if we don't consider one issue with it.\cite{enwiki:1142268588}
        \subsection{Measurement is the Only Mystery}
            The theory of quantum mechanics in it's whole is a non deterministic theory. There exists a wave-function that propagates or evolves through space and time with different evolutions, looked mathematically as operators, that would evolve until we measure it.
            \\
            \\
            The geenral view would be to separate quantum systems behaviour in three steps:
            \begin{enumerate}
                \item \textbf{State:} A system is in it's core a state, described by it's enviorment and it's describtions. To start any quantum process we would need an initial system with an initial state. The state is a mathematical object that we would notationally denote as:
                \begin{equation}
                    \left| \psi_i \right> 
                \end{equation}
                \item \textbf{Evolution:} Interactions and evolutions that a system undergoes would be described by operators. Basically anything that would affect the state of our system, would be an operator acting on the state, thus we denote that:
                \begin{equation}
                    \left|\psi_f\right>  = \hat U \left|\psi_i\right>
                \end{equation}
                Or described with Schrödinger equation:
                \begin{equation}
                    -\hbar\frac{\partial}{\partial t} \psi = \frac{\hbar^2}{2m}\nabla^2 \psi + V(x)\psi 
                \end{equation}
                \item \textbf{Measurement:} The process through which we get information about the system, is fundamentally seen different that the evolution, one might think if we are physical systems then the interaction between us and a quantum system would be described as an operator acting on it. But it seems to be tricky thing that happens fundamentally different than the normal evolutions.
            \end{enumerate}
            The process of measurement is as follows. Suppose you have a wave-function, with measuring you get an answer of say $x$. If you measure this system (which you already measured) again, you would get the same result. As if the measurement has an exact answer. Similar to a classical measurement where the length of a table stays the same, how many times you would try to measure it.
            \\
            \\
            Then suppose you have an ensemble of exact same states, measuring each one of them would give you different answers $x_i$. You therefore for predicting the measurement of a system, is bounded to talk about probabilities of different outcomes. The probability distribution of the appearance of your system at the spatial area $(a,b)$ as an example is shown as:
            \begin{equation}
                P = \int_a^b |\psi|^2 dx 
            \end{equation}
            In Physics (up to measurement in quantum mechanics) Probabilities is used when you are blinded. If the equations are to many and to hard to solve then you would use probabilities, to address some answers and not strugling with the hard equations. Or when the exact interactions are not needed to be known. In thermodynamics for instance we are interested in pressure, temprature, things that doesn't really have adefinition at atomic levels. It is shown that these properties are for macrosopic view and have statistical definitions. 
            \\
            \\
            In measurement of quantum system, on the other hand, we got an intrinsically probabilistic behaviour. But that's not the probelm. The problem is that this type of process has never seen in physical description of nature! The Measurement is not deterministic. From a collapsed wave-function, there's no way to work back to the initial state. Although you are able yo do this with any process in classical and quantum physics (beside the measurement). 
            \\
            \\
            Therefore it is safe to assume that the measurement problem if the problem of describing the first nondeterministic phenomena we encountered. And the only mystery of quantum physics.\cite{ep-qm}
    \section{Summing Up}
    \subsection{Conclusion}
        There we are, describing phenomenas that are deterministic, having a model to describe them, and giving predictions and histories, but at the core of the mathematics that we are using, there exists the theorem that states that not all the statements we are able to create within this formal system is able to have a proof, a model to go backwards.
        \\
        \\
        There can be assumptions, which has been for centuries. That the mathematical models that we are using, are not invented by us, but they are the formal language that the world itself is. The abstractions that Plato would describe. The World can be a subset of the mathematical abstract world. And we are watching the shadows of that abstractions. And then comes the theorems that would describe the abstraction itself, can contain flaws. Flaws in the sense of not being complete. If one accepts or proves that the world is the subset of mathematical universe and abstraction, it is intrinsically obvious to claim that such flaws must exists within the world and therefore within the formal theories we would give about the world.
        \\
        \\
        I would here try to link the dots, and to argue that measurement, can be fundamentally impossible to explain. Not because of hard equations or behaviours that doesn't make sense to us. But because the existence of statements in the world that cannot be modeled is a fundamenta property of the nature itself.
        \subsection{Where to Go?}
        Now there are some questions to be answered in order to achieve any progress in this idea.
        \begin{enumerate}
            \item \textit{Is the world a Formal System?} This question would be the starting point of our investigation. We have to find a way to show if the world is a formal system or not, since it is not trivial to consider the reality a mathematical abstraction.
            \item \textit{How are incompleteness, nondeterminacy and computation limits connected?} We have to show a good mathematical model that how are these things connected to one another.
            \item \textit{Can Measurement be seen as a Gödel incompleteness byproduct?} This question is the next step. We would have to show that measurement is infact the byproduct of the Gödel's incompleteness theorem.
            \item \textit{Applications of nondeterministic processes in computation machines}
            \item \textit{What are physical theories formal system?}
            \item \textit{How can incompleteness show itself through theories and phenomenas}
        \end{enumerate}

            





                        

                


                    \newpage
                    \bibliography{logicsources}
                    \bibliographystyle{plain}
            




\end{document}